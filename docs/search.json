[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "man0ZZZ.github.io",
    "section": "",
    "text": "Classification: comparing performance of different classification models\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\n  \n\n\n\n\nAnomlay detection for a production plant\n\n\n\n\n\n\n\ncode\n\n\nanomaly detection\n\n\nprincipal component analysis\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\n  \n\n\n\n\nClustering Analysis for Smartphone Price Range Prediction\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\n  \n\n\n\n\nLeveraging Odds Ratios for Advanced Keyword Analysis in Email Categorization\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\n  \n\n\n\n\nRegression: tweaking model parameter\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 7, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Classification: comparing performance of different classification models",
    "section": "",
    "text": "In this classification analysis, we aim to predict whether users will make a purchase based on their age and estimated salary. Let’s start by loading the dataset and taking a quick look at the first five rows.\n\n#Loading libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nThe dataset contains information about users, including their age, estimated salary, and whether they made a purchase (‘Purchased’ column).\n\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\nno\n\n\n1\n35\n20000\nno\n\n\n2\n26\n43000\nno\n\n\n3\n27\n57000\nno\n\n\n4\n19\n76000\nno\n\n\n\n\n\n\n\nWe are converting the ‘Purchased’ column into a binary representation for better compatibility with certain algorithms. Using the ‘apply’ method along with a lambda function, we map ‘no’ to 0 (indicating no purchase) and ‘yes’ to 1 (indicating a purchase).\n\ndf_social['Purchased']=df_social['Purchased'].apply(lambda x: 0 if x=='no' else 1)\ndf_social.head(5)\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\n0\n\n\n1\n35\n20000\n0\n\n\n2\n26\n43000\n0\n\n\n3\n27\n57000\n0\n\n\n4\n19\n76000\n0\n\n\n\n\n\n\n\n\nVisualizing the Data:\nWe start by visualizing the relationship between age, estimated salary, and purchase decisions using a scatter plot.\n\nplt.scatter(df_social[df_social['Purchased']==0]['Age'],df_social[df_social['Purchased']==0]['EstimatedSalary'], color='green', label='not purchased')\nplt.scatter(df_social[df_social['Purchased']==1]['Age'],df_social[df_social['Purchased']==1]['EstimatedSalary'], color='blue', label='purchased')\nplt.xlabel('Age')\nplt.ylabel('EstimatedSalary')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x13ca56fd0&gt;\n\n\n\n\n\n\n\nKNN Initial Attempt:\nInitially, we attempted to solve the classification problem using the K-Nearest Neighbors (KNN) algorithm. However, after viewing the decision regions, it became apparent that KNN did not provide a clear separation between the classes.\n\ny=df_social['Purchased'].values\nX=df_social.drop(['Purchased'], axis=1).values\ny=y.ravel()\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=4)\n\nFitting KNN model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_social=KNeighborsClassifier(n_neighbors=10)\nknn_social.fit(X_train,y_train)\nknn_social.score(X_test,y_test)\n\n0.8333333333333334\n\n\n\nfrom mlxtend.plotting import plot_decision_regions\nfig = plot_decision_regions(X_test, y_test,clf=knn_social, legend=2)\nplt.title('KNN')\nplt.show()\n\n\n\n\n\nn_neighbors_values = [4, 10, 12, 15]\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over values for n_neighbors_values\nfor i, n_neighbors in enumerate(n_neighbors_values):\n    knn_social = KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn_social.fit(X_train, y_train)\n    \n    plot_decision_regions(X_test, y_test, clf=knn_social, legend=2, ax=axes[i])\n    \n    axes[i].set_title(f'KNN, n_neighbors={n_neighbors}')\n    \n    score = knn_social.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nExploring Other Models:\nTo find a better model, we decided to explore other classification algorithms such as Decision Tree, Random Forest, and Support Vector Classifier (SVC).\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nmodels = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=10, random_state=42),\n    SVC(kernel='linear', C=1),\n    KNeighborsClassifier(n_neighbors=12)\n]\n\nmodel_names = [\n    'Decision Tree',\n    'Random Forest',\n    'SVC',\n    'KNN'\n]\n\n\n\nComparing Model Performances:\nWe visualized the decision regions and calculated accuracy scores for each model. Notably, the Support Vector Classifier (SVC) stood out with a higher accuracy score and a distinct decision boundary.\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over models\nfor i, (model, model_name) in enumerate(zip(models, model_names)):\n    model.fit(X_train, y_train)\n    plot_decision_regions(X_test, y_test, clf=model, legend=2, ax=axes[i])\n\n    \n    # Set plot title\n    axes[i].set_title(f'{model_name}')\n    \n    # Display the score inside the plot\n    score = model.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nIn our quest to solve the classification problem, we initially attempted KNN but found its decision regions to be less distinct. Upon exploring alternative models, the Support Vector Classifier (SVC) emerged as a strong performer, boasting both a higher accuracy score and a more evident decision boundary. This analysis underscores the importance of model exploration and visualization in the classification process, ultimately leading to the selection of a more effective algorithm for the task at hand."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification: comparing performance of different classification models",
    "section": "",
    "text": "In this classification analysis, we aim to predict whether users will make a purchase based on their age and estimated salary. Let’s start by loading the dataset and taking a quick look at the first five rows.\n\n#Loading libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nThe dataset contains information about users, including their age, estimated salary, and whether they made a purchase (‘Purchased’ column).\n\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\nno\n\n\n1\n35\n20000\nno\n\n\n2\n26\n43000\nno\n\n\n3\n27\n57000\nno\n\n\n4\n19\n76000\nno\n\n\n\n\n\n\n\nWe are converting the ‘Purchased’ column into a binary representation for better compatibility with certain algorithms. Using the ‘apply’ method along with a lambda function, we map ‘no’ to 0 (indicating no purchase) and ‘yes’ to 1 (indicating a purchase).\n\ndf_social['Purchased']=df_social['Purchased'].apply(lambda x: 0 if x=='no' else 1)\ndf_social.head(5)\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\n0\n\n\n1\n35\n20000\n0\n\n\n2\n26\n43000\n0\n\n\n3\n27\n57000\n0\n\n\n4\n19\n76000\n0\n\n\n\n\n\n\n\n\nVisualizing the Data:\nWe start by visualizing the relationship between age, estimated salary, and purchase decisions using a scatter plot.\n\nplt.scatter(df_social[df_social['Purchased']==0]['Age'],df_social[df_social['Purchased']==0]['EstimatedSalary'], color='green', label='not purchased')\nplt.scatter(df_social[df_social['Purchased']==1]['Age'],df_social[df_social['Purchased']==1]['EstimatedSalary'], color='blue', label='purchased')\nplt.xlabel('Age')\nplt.ylabel('EstimatedSalary')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x111f2f8d0&gt;\n\n\n\n\n\n\n\nKNN Initial Attempt:\nInitially, we attempted to solve the classification problem using the K-Nearest Neighbors (KNN) algorithm. However, after viewing the decision regions, it became apparent that KNN did not provide a clear separation between the classes.\n\ny=df_social['Purchased'].values\nX=df_social.drop(['Purchased'], axis=1).values\ny=y.ravel()\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=4)\n\nFitting KNN model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_social=KNeighborsClassifier(n_neighbors=10)\nknn_social.fit(X_train,y_train)\nknn_social.score(X_test,y_test)\n\n0.8333333333333334\n\n\n\nfrom mlxtend.plotting import plot_decision_regions\nfig = plot_decision_regions(X_test, y_test,clf=knn_social, legend=2)\nplt.title('KNN')\nplt.show()\n\n\n\n\n\nn_neighbors_values = [4, 10, 12, 15]\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over values for n_neighbors_values\nfor i, n_neighbors in enumerate(n_neighbors_values):\n    knn_social = KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn_social.fit(X_train, y_train)\n    \n    plot_decision_regions(X_test, y_test, clf=knn_social, legend=2, ax=axes[i])\n    \n    axes[i].set_title(f'KNN, n_neighbors={n_neighbors}')\n    \n    score = knn_social.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nExploring Other Models:\nTo find a better model, we decided to explore other classification algorithms such as Decision Tree, Random Forest, and Support Vector Classifier (SVC).\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nmodels = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=10, random_state=42),\n    SVC(kernel='linear', C=1),\n    KNeighborsClassifier(n_neighbors=12)\n]\n\nmodel_names = [\n    'Decision Tree',\n    'Random Forest',\n    'SVC',\n    'KNN'\n]\n\n\n\nComparing Model Performances:\nWe visualized the decision regions and calculated accuracy scores for each model. Notably, the Support Vector Classifier (SVC) stood out with a higher accuracy score and a distinct decision boundary.\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over models\nfor i, (model, model_name) in enumerate(zip(models, model_names)):\n    model.fit(X_train, y_train)\n    plot_decision_regions(X_test, y_test, clf=model, legend=2, ax=axes[i])\n\n    \n    # Set plot title\n    axes[i].set_title(f'{model_name}')\n    \n    # Display the score inside the plot\n    score = model.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nIn our quest to solve the classification problem, we initially attempted KNN but found its decision regions to be less distinct. Upon exploring alternative models, the Support Vector Classifier (SVC) emerged as a strong performer, boasting both a higher accuracy score and a more evident decision boundary. This analysis underscores the importance of model exploration and visualization in the classification process, ultimately leading to the selection of a more effective algorithm for the task at hand."
  },
  {
    "objectID": "posts/Regression/index.html",
    "href": "posts/Regression/index.html",
    "title": "Regression: tweaking model parameter",
    "section": "",
    "text": "Introduction\nWelcome to a hands-on exploration of solving a regression problem using polynomial regression. In this scenario, we’re dealing with a classic machine learning problem – predicting salaries based on job levels. The dataset includes information about hypothetical job positions and corresponding salaries.\nProblem Statement: The goal here is to build a regression model that accurately predicts salaries. Initially, we attempt a simple linear regression model. However, if the relationship between job levels and salaries is more complex than a straight line, we need a model that can capture these nuances.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\n\n\n\n\nPosition\nLevel\nSalary\n\n\n\n\n0\nBusiness Analyst\n1\n45000\n\n\n1\nJunior Consultant\n2\n50000\n\n\n2\nSenior Consultant\n3\n60000\n\n\n3\nManager\n4\n80000\n\n\n4\nCountry Manager\n5\n110000\n\n\n5\nRegion Manager\n6\n150000\n\n\n6\nPartner\n7\n200000\n\n\n7\nSenior Partner\n8\n300000\n\n\n8\nC-level\n9\n500000\n\n\n9\nCEO\n10\n1000000\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis:\nBefore diving into modeling, it’s crucial to understand the data. The scatter plot visualizes the relationship between job levels and salaries, offering insights into the potential complexity of the underlying patterns.\n\nplt.scatter(df['Level'], df['Salary'])\nplt.xlabel(\"Level\")\nplt.ylabel(\"Salary\")\n\nText(0, 0.5, 'Salary')\n\n\n\n\n\n\n\nLinear Regression:\nTo start, we apply a simple linear regression model, assuming a linear relationship between job levels and salaries. The model aims to minimize the difference between actual and predicted salaries.\n\n#fit linear regression\nfrom sklearn.linear_model import LinearRegression\ny=df.iloc[:,-1].values\nX=df.iloc[:,-2:-1].values\nlin_reg=LinearRegression()\nlin_reg.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\nModel evaluation:\nWe evaluate the performance of the linear regression model using the R-squared metric. A low R-squared value may suggest that a linear model is insufficient for capturing the relationship in the data.\n\ny_pred = lin_reg.predict(X)\n\ndef r2(y, y_pred):\n  tss=0\n  rss=0\n  mean_true = sum(y) / len(y)\n  for true, pred in zip(y, y_pred):\n    tss+=(true-mean_true)**2\n    rss+=(true-pred)**2\n  return 1 - (rss / tss)\n\nr_sq = r2(y, y_pred)\nplt.scatter(df.Level, df.Salary, color='black')\nplt.plot(X, lin_reg.predict(X), color='black')\nplt.text(8.5, 0.75, rf'R$^2$: {r_sq:.2f}', fontsize=10, verticalalignment='top')\n\nText(8.5, 0.75, 'R$^2$: 0.67')\n\n\n\n\n\n\n\nPolynomial Regression and model evaluation:\nRecognizing potential non-linearity, we explore polynomial regression. By transforming our feature variable with different degrees, we can model curved relationships beyond what linear regression allows. We evaluate each polynomial regression model, emphasizing the importance of selecting the right degree for the polynomial features. This process allows us to uncover more complex relationships in the data.\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfig, axs = plt.subplots(3, 1, figsize=(8, 12))\n\n# polynomial regression plots with different degrees\nfor degree, ax in zip([2, 3, 4], axs):\n    # Polynomial features\n    poly_feat = PolynomialFeatures(degree=degree)\n    X_poly = poly_feat.fit_transform(X)\n    \n    # Linear regression\n    linreg_for_polyreg = LinearRegression()\n    linreg_for_polyreg.fit(X_poly, y)\n    \n    # Predicted y values\n    y_poly_pred = linreg_for_polyreg.predict(X_poly)\n    \n    # Plot\n    ax.scatter(X, y, color='black')\n    ax.plot(X, y_poly_pred, color='black')\n    r_sq=r2(y, y_poly_pred)\n    ax.set_title(f'Degree {degree} Polynomial Regression')\n    ax.text(8.5, 0.2e6, rf'R$^2$: {r_sq:.2f}', fontsize=10, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nPredictions and Plotting:\nTo conclude, we compare predictions from both linear and polynomial regression models. This not only demonstrates the flexibility of polynomial regression but also highlights the importance of selecting the appropriate model for a given problem.\n\nx_test=[1.5, 7.8]\n\n#plotting the predicted points\nplt.scatter(X, y, label='Actual Data', color='black')\nplt.scatter(x_test, lin_reg.predict([[1.5], [7.8]]), label='Linear Regression Predictions', marker='x', color='red')\nplt.scatter(x_test, linreg_for_polyreg.predict(poly_feat.fit_transform([[1.5],[7.8]])), label='Polynomial Regression Predictions', marker='o', color='blue')\n\nplt.title('Linear and Polynomial Regression Predictions')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering Analysis for Smartphone Price Range Prediction",
    "section": "",
    "text": "In this blog post, we delve into the fascinating world of clustering to predict smartphone price ranges based on various features. The dataset consists of two parts: the training set (train.csv) containing features and the categorical target variable, “price_range,” and the test set (test.csv) lacking the target column. Our objective is to categorize smartphones from the test set into different clusters using the insights gained from the training set. The dataset can be found on Kaggle\n\nimport pandas as pd\nimport numpy as np\nimport numpy as npd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Clustering/index.html#silhouette-analysis-understanding-cluster-quality",
    "href": "posts/Clustering/index.html#silhouette-analysis-understanding-cluster-quality",
    "title": "Clustering Analysis for Smartphone Price Range Prediction",
    "section": "Silhouette Analysis: Understanding Cluster Quality",
    "text": "Silhouette Analysis: Understanding Cluster Quality\nSilhouette analysis provides insights into the quality and separation of formed clusters. We utilize MiniBatchKMeans and visualize the silhouette scores for different cluster sizes. The code is taken from Scikit learn\n\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport matplotlib.cm as cm\nimport numpy as np\n\n\nrange_n_clusters = [2, 3, 4, 5]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.5, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X_pca) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = MiniBatchKMeans(n_clusters=n_clusters, n_init='auto', random_state=0)\n    cluster_labels = clusterer.fit_predict(X_pca)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X_pca, cluster_labels)\n    print(\n        \"For n_clusters =\",\n        n_clusters,\n        \"The average silhouette_score is :\",\n        silhouette_avg,\n    )\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X_pca, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(\n            np.arange(y_lower, y_upper),\n            0,\n            ith_cluster_silhouette_values,\n            facecolor=color,\n            edgecolor=color,\n            alpha=0.7,\n        )\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.5, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(\n        X_pca[:, 0], X_pca[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n    )\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(\n        centers[:, 0],\n        centers[:, 1],\n        marker=\"o\",\n        c=\"white\",\n        alpha=1,\n        s=200,\n        edgecolor=\"k\",\n    )\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle(\n        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n        % n_clusters,\n        fontsize=14,\n        fontweight=\"bold\",\n    )\n\nplt.show()\n\nFor n_clusters = 2 The average silhouette_score is : 0.3099031312287739\nFor n_clusters = 3 The average silhouette_score is : 0.3760007433407917\nFor n_clusters = 4 The average silhouette_score is : 0.37658001251847223\nFor n_clusters = 5 The average silhouette_score is : 0.3442148747365969"
  },
  {
    "objectID": "posts/Clustering/index.html#elbow-method-determining-optimal-cluster-count",
    "href": "posts/Clustering/index.html#elbow-method-determining-optimal-cluster-count",
    "title": "Clustering Analysis for Smartphone Price Range Prediction",
    "section": "Elbow Method: Determining Optimal Cluster Count",
    "text": "Elbow Method: Determining Optimal Cluster Count\nThe Elbow Method aids in finding the optimal number of clusters. We explore the sum of squared distances for different cluster counts to identify the ‘elbow’ point.\n\nfrom sklearn.cluster import KMeans\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nSum_of_sq_distances = []\nK = range(1,12)\nfor k in K:\n    km = KMeans(n_clusters = k)\n    km = km.fit(X_tr)\n    Sum_of_sq_distances.append(km.inertia_)\nplt.figure(figsize=(15,7))\nplt.plot(K, Sum_of_sq_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/index.html#yellowbrick-elbow-visualizer-an-alternative-visualization",
    "href": "posts/Clustering/index.html#yellowbrick-elbow-visualizer-an-alternative-visualization",
    "title": "Clustering Analysis for Smartphone Price Range Prediction",
    "section": "Yellowbrick Elbow Visualizer: An Alternative Visualization",
    "text": "Yellowbrick Elbow Visualizer: An Alternative Visualization\nThe Yellowbrick library offers an interactive visualizer to assist in determining the optimal number of clusters.\n\nfrom yellowbrick.cluster import KElbowVisualizer\nkmeans = KMeans()\nelbow = KElbowVisualizer(kmeans, k=(1, 15))\nelbow.fit(X)\nelbow.show()\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
  },
  {
    "objectID": "posts/Probability_theory/index.html",
    "href": "posts/Probability_theory/index.html",
    "title": "Leveraging Odds Ratios for Advanced Keyword Analysis in Email Categorization",
    "section": "",
    "text": "Introduction\nIn the vast realm of email communication, distinguishing between legitimate (ham) and unwanted (spam) messages is a constant challenge. Leveraging machine learning and statistical analysis, this blog post embarks on a journey to identify crucial keywords that significantly contribute to the categorization of emails. The ultimate goal is to uncover features that enhance the accuracy of models in predicting spam emails.\n\n\nLoading and Preprocessing the Email Dataset\nOur exploration begins by loading the email dataset and preparing it for analysis. Each email is labeled as either ‘ham’ or ‘spam’, and we aim to extract valuable insights from the text content.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('./spamhamdata.csv', sep = '\\t', header=None)\ndf.columns = ['email_type', 'text']\ndf['label']=df['email_type'].apply(lambda x: 0 if x == 'ham' else 1)\ndf.head(7)\n\n\n\n\n\n\n\n\nemail_type\ntext\nlabel\n\n\n\n\n0\nham\nGo until jurong point, crazy.. Available only ...\n0\n\n\n1\nham\nOk lar... Joking wif u oni...\n0\n\n\n2\nspam\nFree entry in 2 a wkly comp to win FA Cup fina...\n1\n\n\n3\nham\nU dun say so early hor... U c already then say...\n0\n\n\n4\nham\nNah I don't think he goes to usf, he lives aro...\n0\n\n\n5\nspam\nFreeMsg Hey there darling it's been 3 week's n...\n1\n\n\n6\nham\nEven my brother is not like to speak with me. ...\n0\n\n\n\n\n\n\n\n\n\nText Cleaning and Keyword Extraction\nTo identify important keywords, we perform text cleaning by converting words to lowercase, removing numbers, punctuation, and extra spaces. Additionally, we apply lemmatization to obtain the base form of words and remove common English stop words.\n\nimport re\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ndef clean_text(sentence):\n\n  # Converting all words to lower\n  sentence_lowered=sentence.lower()\n\n  # removing puntuations and numbers using regular expression\n  sentence_no_numbers = re.sub(r'\\d+', '', sentence_lowered)\n  sentence_no_punctuations = re.sub(r'[^\\w\\s]', '', sentence_no_numbers)\n  sentence_no_extra_spaces = re.sub(' +', ' ', sentence_no_punctuations)\n\n  # fetched words from nltk package\n  stop = stopwords.words('english')\n\n  stop_words_removed_sentence=[]\n  for k in sentence_no_extra_spaces.split(\" \"):\n    if k not in stop:\n      stop_words_removed_sentence = [lemmatizer.lemmatize(word) for word in sentence_no_extra_spaces.split() if word not in stop and len(word) &gt; 2]\n  return stop_words_removed_sentence\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/apolloos/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/apolloos/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n\n\n\n# example of text cleaning\ntext_sample=\"I am..  ..  $so angry.. that a cu i made this post available\"\nclean_text(text_sample)\n\n['angry', 'made', 'post', 'available']\n\n\n\ndf['keywords']=df['text'].apply(clean_text)\ndf.head(5)\n\n\n\n\n\n\n\n\nemail_type\ntext\nlabel\nkeywords\n\n\n\n\n0\nham\nGo until jurong point, crazy.. Available only ...\n0\n[jurong, point, crazy, available, bugis, great...\n\n\n1\nham\nOk lar... Joking wif u oni...\n0\n[lar, joking, wif, oni]\n\n\n2\nspam\nFree entry in 2 a wkly comp to win FA Cup fina...\n1\n[free, entry, wkly, comp, win, cup, final, tkt...\n\n\n3\nham\nU dun say so early hor... U c already then say...\n0\n[dun, say, early, hor, already, say]\n\n\n4\nham\nNah I don't think he goes to usf, he lives aro...\n0\n[nah, dont, think, go, usf, life, around, though]\n\n\n\n\n\n\n\n\n\nFrequency Analysis of Keywords\nWe analyze the frequency of each word and identify a set of highly repeated words. These words serve as potential candidates for contributing to the categorization process.\n\n#getting the most frequently repeated words\n#| echo: false\nall_words=[]\nfor i in df['keywords']:\n  for j in i:\n    all_words.append(j)\ncount =[]\nfor i in all_words:\n  count.append(all_words.count(i))\ndf_words_count = pd.DataFrame([all_words, count], index = ['words', 'count']).T\ndf_words_count.drop_duplicates(inplace=True)\ndf_words_count.sort_values('count', ascending=False)\n\n\n\n\n\n\n\n\nwords\ncount\n\n\n\n\n93\ncall\n605\n\n\n410\nget\n401\n\n\n40\ndont\n298\n\n\n16\nfree\n278\n\n\n405\nltgt\n276\n\n\n...\n...\n...\n\n\n16257\nvisitor\n1\n\n\n16241\nforwarding\n1\n\n\n16239\nbrilliantly\n1\n\n\n16226\noutreach\n1\n\n\n44521\nbitching\n1\n\n\n\n\n7760 rows × 2 columns\n\n\n\n\n#selecting 149 highly repeated words\ndf_high_freq = df_words_count[df_words_count['count'] &gt; 50]\nhigh_freq_word = list(df_high_freq['words'])\nlen(df_high_freq.index)\n\n149\n\n\n\n# counting instances of occurence of highly repeated words in ham and spam emails\nham=[]\nspam=[]\nfor word in high_freq_word:\n  ham_count=0\n  spam_count=0\n  for ind in range(len(df)):\n    current_row = df.iloc[ind]\n    if word in current_row['keywords']:\n      if current_row['label']==0:\n        ham_count+=1\n      else:\n        spam_count+=1\n  #print(word,ham_count,spam_count)\n  ham.append(ham_count)\n  spam.append(spam_count)\ndf_odds = pd.DataFrame([high_freq_word, ham, spam], index= ['keywords','ham_count','spam_count']).T\ndf_odds = df_odds[df_odds['spam_count'] != 0]\ndf_odds.head(5)\n\n\n\n\n\n\n\n\nkeywords\nham_count\nspam_count\n\n\n\n\n0\ngreat\n93\n11\n\n\n1\ngot\n222\n7\n\n\n2\nwat\n91\n1\n\n\n3\nfree\n58\n169\n\n\n4\nwin\n12\n62\n\n\n\n\n\n\n\n\n\nOdds Ratio and Significance Testing: A Statistical Lens\nNow, we delve into the statistical significance of each keyword using odds ratio and p-values. The odds ratio provides insights into the likelihood of a keyword occurring in spam compared to ham emails. Simultaneously, p-values quantify the evidence against a null hypothesis, helping us assess the significance of the observed differences\n\nimport pandas as pd\nfrom scipy.stats import fisher_exact\n\n# Calculate odds ratio and p-value for each word\nodds_ratios = []\np_values = []\n\nfor index, row in df_odds.iterrows():\n    contingency_table = [\n        [row['spam_count'], row['ham_count']],\n        [len(df.index) - row['spam_count'], len(df.index) - row['ham_count']]\n    ]\n\n    odds_ratio, p_value = fisher_exact(contingency_table)\n    odds_ratios.append(odds_ratio)\n    p_values.append(p_value)\n\n# Add results to the data frame\ndf_odds['Odds_Ratio'] = odds_ratios\ndf_odds['P_Value'] = p_values\n\n# Display the updated data frame\ndf_odds.head(5)\n\n\n\n\n\n\n\n\nkeywords\nham_count\nspam_count\nOdds_Ratio\nP_Value\n\n\n\n\n0\ngreat\n93\n11\n0.116535\n1.838932e-17\n\n\n1\ngot\n222\n7\n0.030313\n1.735920e-57\n\n\n2\nwat\n91\n1\n0.010811\n2.614224e-26\n\n\n3\nfree\n58\n169\n2.973654\n5.124485e-14\n\n\n4\nwin\n12\n62\n5.213551\n2.553913e-09\n\n\n\n\n\n\n\n\n\nBenjamini-Hochberg Correction and Visualization\nThe p-value tests the hypothesis that the odds of the keyword occurring in either ham or spam emails are equal (i.e., 1.0). However, the p-values can easily provide misleading results for two reasons: The p-value does not measure the magnitude of the differences and it raises the risk of false positive rate. To address the issue of false positives, we apply the Benjamini-Hochberg correction. We then visualize the results using a volcano plot, highlighting keywords with substantial odds ratios and low corrected p-values.\n\n# Apply Benjamini-Hochberg correction\nfrom statsmodels.stats.multitest import multipletests\n_, corrected_p_values, _, _ = multipletests(df_odds['P_Value'], method='fdr_bh')\ndf_odds['FDR_Corrected_P_Value'] = corrected_p_values\n\n\n# Create a volcano plot\nplt.figure(figsize=(10, 6))\nplt.scatter(df_odds['Odds_Ratio'], -1 * np.log10(df_odds['FDR_Corrected_P_Value']), color='blue')\n\n# Add labels and title\nplt.xlabel('Odds Ratio')\nplt.ylabel('-log10(FDR Corrected P-Value)')\nplt.title('Volcano Plot of the Keyword analysis')\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\nFinal Selection of Keywords\nBy setting criteria for importance, we narrow down the selection to keywords with odds ratios greater than 5 and FDR-corrected p-values less than 10^-5. These selected keywords can serve as impactful features for enhancing the accuracy of email categorization models.\n\ndf_final = df_odds[(df_odds['Odds_Ratio'] &gt; 5) & (df_odds['FDR_Corrected_P_Value'] &lt; 10**-5)]\nlist(df_final['keywords'])\n\n['win',\n 'txt',\n 'customer',\n 'prize',\n 'claim',\n 'mobile',\n 'cash',\n 'urgent',\n 'nokia',\n 'service',\n 'box',\n 'tone']\n\n\n\n\nConclusion\nIn conclusion, the application of odds ratios and p-values provides a robust statistical foundation for keyword selection. These measures not only quantify the magnitude of differences but also mitigate the risk of false positives. The selected keywords, enriched with statistical significance, can now be employed as powerful features to enhance the accuracy and predictive capabilities of machine learning models in identifying spam emails. Source: Feature Engineering and Selection: A Practical Approach for Predictive Models"
  },
  {
    "objectID": "posts/Anomaly_detection/index.html",
    "href": "posts/Anomaly_detection/index.html",
    "title": "Anomlay detection for a production plant",
    "section": "",
    "text": "Introduction\nIn the complex realm of manufacturing, ensuring the continuous health of machinery is paramount. This blog post delves into the realm of anomaly detection within the context of eight run-to-failure experiments from a production plant. The objective is clear: identify anomalies in machine behavior using a strategic combination of feature selection, autoencoder-based clustering, and quantization errors.\n\n\nLoading and Preprocessing the Datasets\nWe with the loading of datasets from eight run-to-failure experiments. These datasets, capturing the intricate behavior of machines, are meticulously preprocessed to align timestamps and conditions.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n\n\n\n\n\n\n\n\n\nTimestamp\nL_1\nL_2\nA_1\nA_2\nB_1\nB_2\nC_1\nC_2\nA_3\n...\nL_4\nL_5\nL_6\nL_7\nL_8\nL_9\nL_10\nA_5\nB_5\nC_5\n\n\n\n\n0\n0\n-20.470767\n-49.583696\n-49.737023\n-60.125877\n-67.684702\n-67.213115\n-75.635206\n-75.398126\n-45.568737\n...\n-98.610796\n-77.922564\n-65.288020\n-21.577296\n-5.229818\n-20.021259\n-53.399733\n-100.0\n-100.0\n-100.0\n\n\n1\n1\n-4.874715\n-37.094569\n-25.009528\n-35.579886\n-33.459741\n-35.514754\n-33.369469\n-35.644538\n-51.259983\n...\n-98.610796\n-77.922564\n-59.527626\n-8.114661\n-7.114716\n-4.532685\n-52.568426\n-100.0\n-100.0\n-100.0\n\n\n2\n2\n25.132878\n-40.673543\n-25.040018\n-35.461876\n-33.527460\n-35.659016\n-33.274612\n-35.644538\n-56.656587\n...\n-98.610796\n-77.922564\n-67.083467\n22.033657\n-17.198404\n25.123377\n-47.439417\n-100.0\n-100.0\n-100.0\n\n\n3\n3\n-90.508732\n-76.363862\n-25.238204\n-35.592998\n-33.459741\n-35.593443\n-33.396572\n-35.552789\n-52.903776\n...\n-98.610796\n-77.922564\n-98.546543\n-85.325157\n-41.011974\n-91.739428\n-93.192691\n-100.0\n-100.0\n-100.0\n\n\n4\n4\n-95.641610\n-62.582329\n-24.963793\n-35.488101\n-33.459741\n-35.449180\n-33.369469\n-35.552789\n-56.377452\n...\n-98.610796\n-77.922564\n-82.718820\n-92.213349\n-24.248745\n-96.082302\n-70.919928\n-100.0\n-100.0\n-100.0\n\n\n5\n5\n-95.641610\n-52.914129\n-24.963793\n-35.474989\n-33.419110\n-35.580328\n-33.342367\n-35.592110\n-53.973792\n...\n-98.610796\n-77.922564\n-82.718820\n-92.213349\n-24.336295\n-96.082302\n-67.704494\n-100.0\n-100.0\n-100.0\n\n\n6\n6\n-99.422931\n-79.731577\n-24.979038\n-35.658559\n-33.513916\n-35.501639\n-33.383021\n-35.618324\n-54.609599\n...\n-98.610796\n-77.922564\n-80.784439\n-93.639475\n-26.154242\n-97.555235\n-64.818446\n-100.0\n-100.0\n-100.0\n\n\n7\n7\n-99.422931\n-79.731577\n-25.161979\n-35.501213\n-33.378479\n-35.475410\n-33.355918\n-56.982764\n-54.547569\n...\n-98.610796\n-77.922564\n-80.784439\n-93.625214\n-25.438393\n-97.555235\n-61.101090\n-100.0\n-100.0\n-100.0\n\n\n\n\n8 rows × 26 columns\n\n\n\n\nexp_names=[all_datasets[i] for i in range(len(all_datasets)) if i not in ind]\n\n\n\nFeature Selection: Unveiling Significance with ROC Values\nTo identify the most crucial features, we employ a filter method using ROC values. Decision Tree classifiers come into play, predicting machine conditions and ranking features based on univariate ROC values. Filter method of feature selection per feature is employed: build decision tree, predict the target, make prediction rank feature on the basis of machine learning metric (we will use univariate roc values in this problem)\n\n# we divide total timestamps into four equal parts and label each part to describe the condition of the machine\ndef labeler(i):\n  if i&lt;= test_df.shape[0]//4:\n    return 0\n  elif i&gt;test_df.shape[0]//4 and i &lt;= 2*(test_df.shape[0]//4):\n   return 1\n  elif i &gt; 2*(test_df.shape[0]//4) and i &lt;= 3*(test_df.shape[0]//4):\n    return 2\n  else:\n    return 3\nfor i, test_df in enumerate(datasets):\n  test_df['machine_condition']=test_df.Timestamp.apply(labeler)\n  col=test_df.pop('machine_condition')\n  test_df.insert(1, 'machine_condition', col)\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\ndef roc_scores():\n  X_train, X_test, y_train, y_test=train_test_split(test_df.iloc[:,2:], test_df['machine_condition'], train_size=0.8, random_state=0)\n  # univariate roc_auc\n  df_roc_values=pd.DataFrame()\n  roc_values = []\n  for feature in X_test.columns:\n    # if feature=='L_10':\n    dtc=DecisionTreeClassifier()\n    dtc.fit(X_train[feature].to_frame(), y_train)\n    y_pred_prob=dtc.predict_proba(X_test[feature].to_frame())\n\n    #extracting roc values for multiclass output var\n    for class_idx in range(y_pred_prob.shape[1]):\n      class_roc_scores = []\n      y_true_class = (y_test == class_idx).astype(int)  # Convert to binary classification\n      y_pred_class = y_pred_prob[:, class_idx]\n      y_true_class = np.array(y_true_class)\n      y_pred_class = np.array(y_pred_class)\n      class_score=roc_auc_score(y_true_class, y_pred_class)\n      class_roc_scores.append(class_score)\n\n      # print(f\"ROC AUC Score for Class {class_idx}: {auc_score}\")\n\n    # Average ROC AUC score across all classes\n    avg_score = sum(class_roc_scores) / len(class_roc_scores)\n    roc_values.append(avg_score)\n  roc_values=pd.Series(roc_values)\n  return roc_values\n\n\ndf_roc=pd.DataFrame()\nfor i, test_df in enumerate(datasets):\n  roc_series=roc_scores()\n  df_roc=pd.concat([df_roc, roc_series], axis=1)\n  # print(df_roc)\ndf_roc.index=datasets[0].iloc[:,2:].columns\ndf_roc.columns=exp_names\ndf_roc = df_roc.T\ndf_roc.head(5)\n\n\n\n\n\n\n\n\nL_1\nL_2\nA_1\nA_2\nB_1\nB_2\nC_1\nC_2\nA_3\nA_4\n...\nL_4\nL_5\nL_6\nL_7\nL_8\nL_9\nL_10\nA_5\nB_5\nC_5\n\n\n\n\nC11.csv\n0.989719\n0.800622\n0.716381\n0.805171\n0.621604\n0.612124\n0.744775\n0.715891\n0.692415\n0.808485\n...\n0.980403\n0.816665\n0.887565\n0.963003\n0.730728\n0.982077\n0.666120\n0.625770\n0.948750\n0.624316\n\n\nC13-1.csv\n0.989258\n0.835711\n0.629530\n0.790779\n0.805771\n0.766042\n0.547736\n0.768802\n0.642763\n0.707533\n...\n0.887741\n0.985232\n0.774679\n0.635377\n0.548002\n0.911593\n0.677480\n0.670073\n0.665349\n0.659305\n\n\nC14.csv\n0.992529\n0.781165\n0.557617\n0.687565\n0.687223\n0.689019\n0.660716\n0.690027\n0.533338\n0.686555\n...\n0.988066\n0.894792\n0.896809\n0.685654\n0.540606\n0.904820\n0.693794\n0.831570\n0.807473\n0.852070\n\n\nC15.csv\n0.993361\n0.931253\n0.554749\n0.858519\n0.638526\n0.862530\n0.546365\n0.662041\n0.774243\n0.836862\n...\n0.842338\n0.634486\n0.986360\n0.649579\n0.549271\n0.902103\n0.573459\n0.571417\n0.627396\n0.593993\n\n\nC16.csv\n0.992478\n0.842233\n0.874162\n0.840800\n0.605960\n0.840122\n0.879170\n0.707794\n0.858725\n0.817181\n...\n0.950300\n0.795837\n0.942829\n0.653904\n0.537704\n0.926012\n0.575221\n0.704839\n0.627433\n0.703330\n\n\n\n\n5 rows × 25 columns\n\n\n\n\n# selecting features with highest average roc score across all run-to-failure experiments\ndf_roc.mean().sort_values(ascending=False)[0:5]\n\nL_1    0.993399\nL_9    0.937474\nL_3    0.933457\nL_4    0.927554\nL_6    0.893747\ndtype: float64\n\n\n\n\nAutoencoder for Anomaly Detection\nWith selected features in hand, the journey continues into the realm of autoencoders. Utilizing PCA as the foundation, we build an autoencoder to capture the essence of machine behavior.\n\nfrom sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nfor i, data in enumerate(datasets):\n  data.iloc[:,2:]=ss.fit_transform(data.iloc[:,2:])\n\n\ndatasets_autoencoder=[]\nfor i,data in enumerate(datasets):\n  data=data.loc[:,['Timestamp','L_1','L_9','L_3','L_4','L_6']]\n  datasets_autoencoder.append(data)\n\n\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\n\n\n# we are not using train test split, because we want the representative data when machine is in best condition\n# so we select first 5% timestamp of every experiments\ntraining_data = np.vstack([data.iloc[:int(0.05 * len(data)),1:] for k,data in enumerate(datasets_autoencoder)])\n\ntransformed_data = pca.fit_transform(training_data)\n\n# Calculate reconstruction errors for train_data\nreconstructed_data = pca.inverse_transform(transformed_data)\nreconstruction_errors = np.mean(np.square(training_data - reconstructed_data), axis = 1)\n\n\n\nSetting the Anomaly Detection Threshold\nA crucial step in anomaly detection involves setting a threshold. We employ quantization errors to establish this threshold, a key parameter in distinguishing normal machine behavior from anomalies.\n\n# Setting threshold for anomaly detection, in real-world is set manually based on the previous data of the machines\nthreshold = np.percentile(reconstruction_errors, 99.99)\nprint('threshold for anomaly detection:', threshold)\n\nthreshold for anomaly detection: 2.2508852337037815\n\n\n\n\nAnomaly Detection in Action\nWith the stage set, we deploy our autoencoder on all run-to-failure datasets. The quantization errors are analyzed to pinpoint timestamps where anomalies in machine behavior occur.\n\nfor i, data_set in enumerate(datasets_autoencoder):\n  print('                                                                    ')\n  print('                                                                    ')\n  print('=====================dataset',exp_names[i],'========================')\n  # Detect anomalies in new_data\n  transformed_data = pca.fit_transform(data_set.iloc[:,1:].values)\n  new_data_reconstructed = pca.inverse_transform(transformed_data)\n  new_data_errors = np.mean(np.square(data_set.iloc[:,1:].values - new_data_reconstructed), axis=1)\n  #print(len(new_data_errors)==data_set.shape[0])\n  print('timestamp duration: 0 to', data_set.shape[0])\n  anomaly_timestamp=[]\n  for j in range(len(new_data_errors)):\n    if new_data_errors[j] &gt; threshold:\n      anomaly_timestamp.append(j)\n  plt.plot(range(len(data_set.index)), new_data_errors)\n  plt.axhline(y=threshold, color='red', linestyle='--', label='anomaly_threshold'.format(threshold))\n  plt.xlabel('Timestamp')\n  plt.ylabel('Quantization errors')\n  plt.title('Run-to-failure experiment '+exp_names[i])\n  plt.legend()\n  plt.show()\n\n  if len(anomaly_timestamp)&lt;=1:\n    print('No anomaly detected')\n    continue\n  else:\n    for k in range(len(anomaly_timestamp)-1):\n      if anomaly_timestamp[k+1]-anomaly_timestamp[k]&lt;=10:\n        print('First instance of anomaly detected at Timestamp:', anomaly_timestamp[k])\n        break\n\n                                                                    \n                                                                    \n=====================dataset C11.csv ========================\ntimestamp duration: 0 to 18429\nNo anomaly detected\n                                                                    \n                                                                    \n=====================dataset C13-1.csv ========================\ntimestamp duration: 0 to 23670\nFirst instance of anomaly detected at Timestamp: 18737\n                                                                    \n                                                                    \n=====================dataset C14.csv ========================\ntimestamp duration: 0 to 32848\nFirst instance of anomaly detected at Timestamp: 28477\n                                                                    \n                                                                    \n=====================dataset C15.csv ========================\ntimestamp duration: 0 to 30737\nFirst instance of anomaly detected at Timestamp: 16244\n                                                                    \n                                                                    \n=====================dataset C16.csv ========================\ntimestamp duration: 0 to 21021\nNo anomaly detected\n                                                                    \n                                                                    \n=====================dataset C7-1.csv ========================\ntimestamp duration: 0 to 51671\nFirst instance of anomaly detected at Timestamp: 31657\n                                                                    \n                                                                    \n=====================dataset C8.csv ========================\ntimestamp duration: 0 to 15803\nNo anomaly detected\n                                                                    \n                                                                    \n=====================dataset C9.csv ========================\ntimestamp duration: 0 to 34245\nFirst instance of anomaly detected at Timestamp: 13702\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConclusion\nThis blog post has navigated through the intricate process of anomaly detection in machine health. From feature selection using ROC values to the implementation of autoencoders and quantization errors, each step contributes to a comprehensive approach in identifying anomalies. The strategies presented here serve as a valuable toolkit for practitioners seeking to enhance machinery monitoring and predictive maintenance in industrial settings."
  }
]