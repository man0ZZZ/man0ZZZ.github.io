[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "man0ZZZ.github.io",
    "section": "",
    "text": "Classification: comparing performance of different classification models\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\n  \n\n\n\n\nAnomlay detection for a production plant\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\n  \n\n\n\n\nClustering with K means clustering algorithm\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\n  \n\n\n\n\nRegression\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Classification: comparing performance of different classification models",
    "section": "",
    "text": "In this classification analysis, we aim to predict whether users will make a purchase based on their age and estimated salary. Let’s start by loading the dataset and taking a quick look at the first five rows.\n\n#Loading libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nThe dataset contains information about users, including their age, estimated salary, and whether they made a purchase (‘Purchased’ column).\n\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\nno\n\n\n1\n35\n20000\nno\n\n\n2\n26\n43000\nno\n\n\n3\n27\n57000\nno\n\n\n4\n19\n76000\nno\n\n\n\n\n\n\n\nWe are converting the ‘Purchased’ column into a binary representation for better compatibility with certain algorithms. Using the ‘apply’ method along with a lambda function, we map ‘no’ to 0 (indicating no purchase) and ‘yes’ to 1 (indicating a purchase).\n\ndf_social['Purchased']=df_social['Purchased'].apply(lambda x: 0 if x=='no' else 1)\ndf_social.head(5)\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\n0\n\n\n1\n35\n20000\n0\n\n\n2\n26\n43000\n0\n\n\n3\n27\n57000\n0\n\n\n4\n19\n76000\n0\n\n\n\n\n\n\n\n\nVisualizing the Data:\nWe start by visualizing the relationship between age, estimated salary, and purchase decisions using a scatter plot.\n\nplt.scatter(df_social[df_social['Purchased']==0]['Age'],df_social[df_social['Purchased']==0]['EstimatedSalary'], color='green', label='not purchased')\nplt.scatter(df_social[df_social['Purchased']==1]['Age'],df_social[df_social['Purchased']==1]['EstimatedSalary'], color='blue', label='purchased')\nplt.xlabel('Age')\nplt.ylabel('EstimatedSalary')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x13ca56fd0&gt;\n\n\n\n\n\n\n\nKNN Initial Attempt:\nInitially, we attempted to solve the classification problem using the K-Nearest Neighbors (KNN) algorithm. However, after viewing the decision regions, it became apparent that KNN did not provide a clear separation between the classes.\n\ny=df_social['Purchased'].values\nX=df_social.drop(['Purchased'], axis=1).values\ny=y.ravel()\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=4)\n\nFitting KNN model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_social=KNeighborsClassifier(n_neighbors=10)\nknn_social.fit(X_train,y_train)\nknn_social.score(X_test,y_test)\n\n0.8333333333333334\n\n\n\nfrom mlxtend.plotting import plot_decision_regions\nfig = plot_decision_regions(X_test, y_test,clf=knn_social, legend=2)\nplt.title('KNN')\nplt.show()\n\n\n\n\n\nn_neighbors_values = [4, 10, 12, 15]\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over values for n_neighbors_values\nfor i, n_neighbors in enumerate(n_neighbors_values):\n    knn_social = KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn_social.fit(X_train, y_train)\n    \n    plot_decision_regions(X_test, y_test, clf=knn_social, legend=2, ax=axes[i])\n    \n    axes[i].set_title(f'KNN, n_neighbors={n_neighbors}')\n    \n    score = knn_social.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nExploring Other Models:\nTo find a better model, we decided to explore other classification algorithms such as Decision Tree, Random Forest, and Support Vector Classifier (SVC).\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nmodels = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=10, random_state=42),\n    SVC(kernel='linear', C=1),\n    KNeighborsClassifier(n_neighbors=12)\n]\n\nmodel_names = [\n    'Decision Tree',\n    'Random Forest',\n    'SVC',\n    'KNN'\n]\n\n\n\nComparing Model Performances:\nWe visualized the decision regions and calculated accuracy scores for each model. Notably, the Support Vector Classifier (SVC) stood out with a higher accuracy score and a distinct decision boundary.\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over models\nfor i, (model, model_name) in enumerate(zip(models, model_names)):\n    model.fit(X_train, y_train)\n    plot_decision_regions(X_test, y_test, clf=model, legend=2, ax=axes[i])\n\n    \n    # Set plot title\n    axes[i].set_title(f'{model_name}')\n    \n    # Display the score inside the plot\n    score = model.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nIn our quest to solve the classification problem, we initially attempted KNN but found its decision regions to be less distinct. Upon exploring alternative models, the Support Vector Classifier (SVC) emerged as a strong performer, boasting both a higher accuracy score and a more evident decision boundary. This analysis underscores the importance of model exploration and visualization in the classification process, ultimately leading to the selection of a more effective algorithm for the task at hand."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification: comparing performance of different classification models",
    "section": "",
    "text": "In this classification analysis, we aim to predict whether users will make a purchase based on their age and estimated salary. Let’s start by loading the dataset and taking a quick look at the first five rows.\n\n#Loading libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nThe dataset contains information about users, including their age, estimated salary, and whether they made a purchase (‘Purchased’ column).\n\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\nno\n\n\n1\n35\n20000\nno\n\n\n2\n26\n43000\nno\n\n\n3\n27\n57000\nno\n\n\n4\n19\n76000\nno\n\n\n\n\n\n\n\nWe are converting the ‘Purchased’ column into a binary representation for better compatibility with certain algorithms. Using the ‘apply’ method along with a lambda function, we map ‘no’ to 0 (indicating no purchase) and ‘yes’ to 1 (indicating a purchase).\n\ndf_social['Purchased']=df_social['Purchased'].apply(lambda x: 0 if x=='no' else 1)\ndf_social.head(5)\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\n0\n\n\n1\n35\n20000\n0\n\n\n2\n26\n43000\n0\n\n\n3\n27\n57000\n0\n\n\n4\n19\n76000\n0\n\n\n\n\n\n\n\n\nVisualizing the Data:\nWe start by visualizing the relationship between age, estimated salary, and purchase decisions using a scatter plot.\n\nplt.scatter(df_social[df_social['Purchased']==0]['Age'],df_social[df_social['Purchased']==0]['EstimatedSalary'], color='green', label='not purchased')\nplt.scatter(df_social[df_social['Purchased']==1]['Age'],df_social[df_social['Purchased']==1]['EstimatedSalary'], color='blue', label='purchased')\nplt.xlabel('Age')\nplt.ylabel('EstimatedSalary')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x111f2f8d0&gt;\n\n\n\n\n\n\n\nKNN Initial Attempt:\nInitially, we attempted to solve the classification problem using the K-Nearest Neighbors (KNN) algorithm. However, after viewing the decision regions, it became apparent that KNN did not provide a clear separation between the classes.\n\ny=df_social['Purchased'].values\nX=df_social.drop(['Purchased'], axis=1).values\ny=y.ravel()\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=4)\n\nFitting KNN model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_social=KNeighborsClassifier(n_neighbors=10)\nknn_social.fit(X_train,y_train)\nknn_social.score(X_test,y_test)\n\n0.8333333333333334\n\n\n\nfrom mlxtend.plotting import plot_decision_regions\nfig = plot_decision_regions(X_test, y_test,clf=knn_social, legend=2)\nplt.title('KNN')\nplt.show()\n\n\n\n\n\nn_neighbors_values = [4, 10, 12, 15]\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over values for n_neighbors_values\nfor i, n_neighbors in enumerate(n_neighbors_values):\n    knn_social = KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn_social.fit(X_train, y_train)\n    \n    plot_decision_regions(X_test, y_test, clf=knn_social, legend=2, ax=axes[i])\n    \n    axes[i].set_title(f'KNN, n_neighbors={n_neighbors}')\n    \n    score = knn_social.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nExploring Other Models:\nTo find a better model, we decided to explore other classification algorithms such as Decision Tree, Random Forest, and Support Vector Classifier (SVC).\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nmodels = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=10, random_state=42),\n    SVC(kernel='linear', C=1),\n    KNeighborsClassifier(n_neighbors=12)\n]\n\nmodel_names = [\n    'Decision Tree',\n    'Random Forest',\n    'SVC',\n    'KNN'\n]\n\n\n\nComparing Model Performances:\nWe visualized the decision regions and calculated accuracy scores for each model. Notably, the Support Vector Classifier (SVC) stood out with a higher accuracy score and a distinct decision boundary.\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over models\nfor i, (model, model_name) in enumerate(zip(models, model_names)):\n    model.fit(X_train, y_train)\n    plot_decision_regions(X_test, y_test, clf=model, legend=2, ax=axes[i])\n\n    \n    # Set plot title\n    axes[i].set_title(f'{model_name}')\n    \n    # Display the score inside the plot\n    score = model.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nIn our quest to solve the classification problem, we initially attempted KNN but found its decision regions to be less distinct. Upon exploring alternative models, the Support Vector Classifier (SVC) emerged as a strong performer, boasting both a higher accuracy score and a more evident decision boundary. This analysis underscores the importance of model exploration and visualization in the classification process, ultimately leading to the selection of a more effective algorithm for the task at hand."
  }
]