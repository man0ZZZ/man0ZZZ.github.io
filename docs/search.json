[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "man0ZZZ.github.io",
    "section": "",
    "text": "Classification: comparing performance of different classification models\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\n  \n\n\n\n\nAnomlay detection for a production plant\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\n  \n\n\n\n\nClustering with K means clustering algorithm\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\n  \n\n\n\n\nRegression: tweaking model parameter\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Classification: comparing performance of different classification models",
    "section": "",
    "text": "In this classification analysis, we aim to predict whether users will make a purchase based on their age and estimated salary. Let’s start by loading the dataset and taking a quick look at the first five rows.\n\n#Loading libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nThe dataset contains information about users, including their age, estimated salary, and whether they made a purchase (‘Purchased’ column).\n\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\nno\n\n\n1\n35\n20000\nno\n\n\n2\n26\n43000\nno\n\n\n3\n27\n57000\nno\n\n\n4\n19\n76000\nno\n\n\n\n\n\n\n\nWe are converting the ‘Purchased’ column into a binary representation for better compatibility with certain algorithms. Using the ‘apply’ method along with a lambda function, we map ‘no’ to 0 (indicating no purchase) and ‘yes’ to 1 (indicating a purchase).\n\ndf_social['Purchased']=df_social['Purchased'].apply(lambda x: 0 if x=='no' else 1)\ndf_social.head(5)\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\n0\n\n\n1\n35\n20000\n0\n\n\n2\n26\n43000\n0\n\n\n3\n27\n57000\n0\n\n\n4\n19\n76000\n0\n\n\n\n\n\n\n\n\nVisualizing the Data:\nWe start by visualizing the relationship between age, estimated salary, and purchase decisions using a scatter plot.\n\nplt.scatter(df_social[df_social['Purchased']==0]['Age'],df_social[df_social['Purchased']==0]['EstimatedSalary'], color='green', label='not purchased')\nplt.scatter(df_social[df_social['Purchased']==1]['Age'],df_social[df_social['Purchased']==1]['EstimatedSalary'], color='blue', label='purchased')\nplt.xlabel('Age')\nplt.ylabel('EstimatedSalary')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x13ca56fd0&gt;\n\n\n\n\n\n\n\nKNN Initial Attempt:\nInitially, we attempted to solve the classification problem using the K-Nearest Neighbors (KNN) algorithm. However, after viewing the decision regions, it became apparent that KNN did not provide a clear separation between the classes.\n\ny=df_social['Purchased'].values\nX=df_social.drop(['Purchased'], axis=1).values\ny=y.ravel()\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=4)\n\nFitting KNN model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_social=KNeighborsClassifier(n_neighbors=10)\nknn_social.fit(X_train,y_train)\nknn_social.score(X_test,y_test)\n\n0.8333333333333334\n\n\n\nfrom mlxtend.plotting import plot_decision_regions\nfig = plot_decision_regions(X_test, y_test,clf=knn_social, legend=2)\nplt.title('KNN')\nplt.show()\n\n\n\n\n\nn_neighbors_values = [4, 10, 12, 15]\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over values for n_neighbors_values\nfor i, n_neighbors in enumerate(n_neighbors_values):\n    knn_social = KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn_social.fit(X_train, y_train)\n    \n    plot_decision_regions(X_test, y_test, clf=knn_social, legend=2, ax=axes[i])\n    \n    axes[i].set_title(f'KNN, n_neighbors={n_neighbors}')\n    \n    score = knn_social.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nExploring Other Models:\nTo find a better model, we decided to explore other classification algorithms such as Decision Tree, Random Forest, and Support Vector Classifier (SVC).\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nmodels = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=10, random_state=42),\n    SVC(kernel='linear', C=1),\n    KNeighborsClassifier(n_neighbors=12)\n]\n\nmodel_names = [\n    'Decision Tree',\n    'Random Forest',\n    'SVC',\n    'KNN'\n]\n\n\n\nComparing Model Performances:\nWe visualized the decision regions and calculated accuracy scores for each model. Notably, the Support Vector Classifier (SVC) stood out with a higher accuracy score and a distinct decision boundary.\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over models\nfor i, (model, model_name) in enumerate(zip(models, model_names)):\n    model.fit(X_train, y_train)\n    plot_decision_regions(X_test, y_test, clf=model, legend=2, ax=axes[i])\n\n    \n    # Set plot title\n    axes[i].set_title(f'{model_name}')\n    \n    # Display the score inside the plot\n    score = model.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nIn our quest to solve the classification problem, we initially attempted KNN but found its decision regions to be less distinct. Upon exploring alternative models, the Support Vector Classifier (SVC) emerged as a strong performer, boasting both a higher accuracy score and a more evident decision boundary. This analysis underscores the importance of model exploration and visualization in the classification process, ultimately leading to the selection of a more effective algorithm for the task at hand."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification: comparing performance of different classification models",
    "section": "",
    "text": "In this classification analysis, we aim to predict whether users will make a purchase based on their age and estimated salary. Let’s start by loading the dataset and taking a quick look at the first five rows.\n\n#Loading libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nThe dataset contains information about users, including their age, estimated salary, and whether they made a purchase (‘Purchased’ column).\n\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\nno\n\n\n1\n35\n20000\nno\n\n\n2\n26\n43000\nno\n\n\n3\n27\n57000\nno\n\n\n4\n19\n76000\nno\n\n\n\n\n\n\n\nWe are converting the ‘Purchased’ column into a binary representation for better compatibility with certain algorithms. Using the ‘apply’ method along with a lambda function, we map ‘no’ to 0 (indicating no purchase) and ‘yes’ to 1 (indicating a purchase).\n\ndf_social['Purchased']=df_social['Purchased'].apply(lambda x: 0 if x=='no' else 1)\ndf_social.head(5)\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\n0\n\n\n1\n35\n20000\n0\n\n\n2\n26\n43000\n0\n\n\n3\n27\n57000\n0\n\n\n4\n19\n76000\n0\n\n\n\n\n\n\n\n\nVisualizing the Data:\nWe start by visualizing the relationship between age, estimated salary, and purchase decisions using a scatter plot.\n\nplt.scatter(df_social[df_social['Purchased']==0]['Age'],df_social[df_social['Purchased']==0]['EstimatedSalary'], color='green', label='not purchased')\nplt.scatter(df_social[df_social['Purchased']==1]['Age'],df_social[df_social['Purchased']==1]['EstimatedSalary'], color='blue', label='purchased')\nplt.xlabel('Age')\nplt.ylabel('EstimatedSalary')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x111f2f8d0&gt;\n\n\n\n\n\n\n\nKNN Initial Attempt:\nInitially, we attempted to solve the classification problem using the K-Nearest Neighbors (KNN) algorithm. However, after viewing the decision regions, it became apparent that KNN did not provide a clear separation between the classes.\n\ny=df_social['Purchased'].values\nX=df_social.drop(['Purchased'], axis=1).values\ny=y.ravel()\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=4)\n\nFitting KNN model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_social=KNeighborsClassifier(n_neighbors=10)\nknn_social.fit(X_train,y_train)\nknn_social.score(X_test,y_test)\n\n0.8333333333333334\n\n\n\nfrom mlxtend.plotting import plot_decision_regions\nfig = plot_decision_regions(X_test, y_test,clf=knn_social, legend=2)\nplt.title('KNN')\nplt.show()\n\n\n\n\n\nn_neighbors_values = [4, 10, 12, 15]\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over values for n_neighbors_values\nfor i, n_neighbors in enumerate(n_neighbors_values):\n    knn_social = KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn_social.fit(X_train, y_train)\n    \n    plot_decision_regions(X_test, y_test, clf=knn_social, legend=2, ax=axes[i])\n    \n    axes[i].set_title(f'KNN, n_neighbors={n_neighbors}')\n    \n    score = knn_social.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nExploring Other Models:\nTo find a better model, we decided to explore other classification algorithms such as Decision Tree, Random Forest, and Support Vector Classifier (SVC).\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nmodels = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=10, random_state=42),\n    SVC(kernel='linear', C=1),\n    KNeighborsClassifier(n_neighbors=12)\n]\n\nmodel_names = [\n    'Decision Tree',\n    'Random Forest',\n    'SVC',\n    'KNN'\n]\n\n\n\nComparing Model Performances:\nWe visualized the decision regions and calculated accuracy scores for each model. Notably, the Support Vector Classifier (SVC) stood out with a higher accuracy score and a distinct decision boundary.\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over models\nfor i, (model, model_name) in enumerate(zip(models, model_names)):\n    model.fit(X_train, y_train)\n    plot_decision_regions(X_test, y_test, clf=model, legend=2, ax=axes[i])\n\n    \n    # Set plot title\n    axes[i].set_title(f'{model_name}')\n    \n    # Display the score inside the plot\n    score = model.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nIn our quest to solve the classification problem, we initially attempted KNN but found its decision regions to be less distinct. Upon exploring alternative models, the Support Vector Classifier (SVC) emerged as a strong performer, boasting both a higher accuracy score and a more evident decision boundary. This analysis underscores the importance of model exploration and visualization in the classification process, ultimately leading to the selection of a more effective algorithm for the task at hand."
  },
  {
    "objectID": "posts/Regression/index.html",
    "href": "posts/Regression/index.html",
    "title": "Regression: tweaking model parameter",
    "section": "",
    "text": "Introduction\nWelcome to a hands-on exploration of solving a regression problem using polynomial regression. In this scenario, we’re dealing with a classic machine learning problem – predicting salaries based on job levels. The dataset includes information about hypothetical job positions and corresponding salaries.\nProblem Statement: The goal here is to build a regression model that accurately predicts salaries. Initially, we attempt a simple linear regression model. However, if the relationship between job levels and salaries is more complex than a straight line, we need a model that can capture these nuances.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\n\n\n\n\nPosition\nLevel\nSalary\n\n\n\n\n0\nBusiness Analyst\n1\n45000\n\n\n1\nJunior Consultant\n2\n50000\n\n\n2\nSenior Consultant\n3\n60000\n\n\n3\nManager\n4\n80000\n\n\n4\nCountry Manager\n5\n110000\n\n\n5\nRegion Manager\n6\n150000\n\n\n6\nPartner\n7\n200000\n\n\n7\nSenior Partner\n8\n300000\n\n\n8\nC-level\n9\n500000\n\n\n9\nCEO\n10\n1000000\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis:\nBefore diving into modeling, it’s crucial to understand the data. The scatter plot visualizes the relationship between job levels and salaries, offering insights into the potential complexity of the underlying patterns.\n\nplt.scatter(df['Level'], df['Salary'])\nplt.xlabel(\"Level\")\nplt.ylabel(\"Salary\")\n\nText(0, 0.5, 'Salary')\n\n\n\n\n\n\n\nLinear Regression:\nTo start, we apply a simple linear regression model, assuming a linear relationship between job levels and salaries. The model aims to minimize the difference between actual and predicted salaries.\n\n#fit linear regression\nfrom sklearn.linear_model import LinearRegression\ny=df.iloc[:,-1].values\nX=df.iloc[:,-2:-1].values\nlin_reg=LinearRegression()\nlin_reg.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\nModel evaluation:\nWe evaluate the performance of the linear regression model using the R-squared metric. A low R-squared value may suggest that a linear model is insufficient for capturing the relationship in the data.\n\ny_pred = lin_reg.predict(X)\n\ndef r2(y, y_pred):\n  tss=0\n  rss=0\n  mean_true = sum(y) / len(y)\n  for true, pred in zip(y, y_pred):\n    tss+=(true-mean_true)**2\n    rss+=(true-pred)**2\n  return 1 - (rss / tss)\n\nr_sq = r2(y, y_pred)\nplt.scatter(df.Level, df.Salary, color='black')\nplt.plot(X, lin_reg.predict(X), color='black')\nplt.text(8.5, 0.75, rf'R$^2$: {r_sq:.2f}', fontsize=10, verticalalignment='top')\n\nText(8.5, 0.75, 'R$^2$: 0.67')\n\n\n\n\n\n\n\nPolynomial Regression and model evaluation:\nRecognizing potential non-linearity, we explore polynomial regression. By transforming our feature variable with different degrees, we can model curved relationships beyond what linear regression allows. We evaluate each polynomial regression model, emphasizing the importance of selecting the right degree for the polynomial features. This process allows us to uncover more complex relationships in the data.\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfig, axs = plt.subplots(3, 1, figsize=(8, 12))\n\n# polynomial regression plots with different degrees\nfor degree, ax in zip([2, 3, 4], axs):\n    # Polynomial features\n    poly_feat = PolynomialFeatures(degree=degree)\n    X_poly = poly_feat.fit_transform(X)\n    \n    # Linear regression\n    linreg_for_polyreg = LinearRegression()\n    linreg_for_polyreg.fit(X_poly, y)\n    \n    # Predicted y values\n    y_poly_pred = linreg_for_polyreg.predict(X_poly)\n    \n    # Plot\n    ax.scatter(X, y, color='black')\n    ax.plot(X, y_poly_pred, color='black')\n    r_sq=r2(y, y_poly_pred)\n    ax.set_title(f'Degree {degree} Polynomial Regression')\n    ax.text(8.5, 0.2e6, rf'R$^2$: {r_sq:.2f}', fontsize=10, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nPredictions and Plotting:\nTo conclude, we compare predictions from both linear and polynomial regression models. This not only demonstrates the flexibility of polynomial regression but also highlights the importance of selecting the appropriate model for a given problem.\n\nx_test=[1.5, 7.8]\n\n#plotting the predicted points\nplt.scatter(X, y, label='Actual Data', color='black')\nplt.scatter(x_test, lin_reg.predict([[1.5], [7.8]]), label='Linear Regression Predictions', marker='x', color='red')\nplt.scatter(x_test, linreg_for_polyreg.predict(poly_feat.fit_transform([[1.5],[7.8]])), label='Polynomial Regression Predictions', marker='o', color='blue')\n\nplt.title('Linear and Polynomial Regression Predictions')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\nplt.show()"
  }
]