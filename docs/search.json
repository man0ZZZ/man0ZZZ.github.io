[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "man0ZZZ.github.io",
    "section": "",
    "text": "Classification: comparing performance of different classification models\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\n  \n\n\n\n\nAnomlay detection for a production plant\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\n  \n\n\n\n\nClustering Analysis for Smartphone Price Range Prediction\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\n  \n\n\n\n\nRegression: tweaking model parameter\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nDec 6, 2023\n\n\nManoj Subedi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Classification: comparing performance of different classification models",
    "section": "",
    "text": "In this classification analysis, we aim to predict whether users will make a purchase based on their age and estimated salary. Let’s start by loading the dataset and taking a quick look at the first five rows.\n\n#Loading libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nThe dataset contains information about users, including their age, estimated salary, and whether they made a purchase (‘Purchased’ column).\n\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\nno\n\n\n1\n35\n20000\nno\n\n\n2\n26\n43000\nno\n\n\n3\n27\n57000\nno\n\n\n4\n19\n76000\nno\n\n\n\n\n\n\n\nWe are converting the ‘Purchased’ column into a binary representation for better compatibility with certain algorithms. Using the ‘apply’ method along with a lambda function, we map ‘no’ to 0 (indicating no purchase) and ‘yes’ to 1 (indicating a purchase).\n\ndf_social['Purchased']=df_social['Purchased'].apply(lambda x: 0 if x=='no' else 1)\ndf_social.head(5)\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\n0\n\n\n1\n35\n20000\n0\n\n\n2\n26\n43000\n0\n\n\n3\n27\n57000\n0\n\n\n4\n19\n76000\n0\n\n\n\n\n\n\n\n\nVisualizing the Data:\nWe start by visualizing the relationship between age, estimated salary, and purchase decisions using a scatter plot.\n\nplt.scatter(df_social[df_social['Purchased']==0]['Age'],df_social[df_social['Purchased']==0]['EstimatedSalary'], color='green', label='not purchased')\nplt.scatter(df_social[df_social['Purchased']==1]['Age'],df_social[df_social['Purchased']==1]['EstimatedSalary'], color='blue', label='purchased')\nplt.xlabel('Age')\nplt.ylabel('EstimatedSalary')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x13ca56fd0&gt;\n\n\n\n\n\n\n\nKNN Initial Attempt:\nInitially, we attempted to solve the classification problem using the K-Nearest Neighbors (KNN) algorithm. However, after viewing the decision regions, it became apparent that KNN did not provide a clear separation between the classes.\n\ny=df_social['Purchased'].values\nX=df_social.drop(['Purchased'], axis=1).values\ny=y.ravel()\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=4)\n\nFitting KNN model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_social=KNeighborsClassifier(n_neighbors=10)\nknn_social.fit(X_train,y_train)\nknn_social.score(X_test,y_test)\n\n0.8333333333333334\n\n\n\nfrom mlxtend.plotting import plot_decision_regions\nfig = plot_decision_regions(X_test, y_test,clf=knn_social, legend=2)\nplt.title('KNN')\nplt.show()\n\n\n\n\n\nn_neighbors_values = [4, 10, 12, 15]\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over values for n_neighbors_values\nfor i, n_neighbors in enumerate(n_neighbors_values):\n    knn_social = KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn_social.fit(X_train, y_train)\n    \n    plot_decision_regions(X_test, y_test, clf=knn_social, legend=2, ax=axes[i])\n    \n    axes[i].set_title(f'KNN, n_neighbors={n_neighbors}')\n    \n    score = knn_social.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nExploring Other Models:\nTo find a better model, we decided to explore other classification algorithms such as Decision Tree, Random Forest, and Support Vector Classifier (SVC).\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nmodels = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=10, random_state=42),\n    SVC(kernel='linear', C=1),\n    KNeighborsClassifier(n_neighbors=12)\n]\n\nmodel_names = [\n    'Decision Tree',\n    'Random Forest',\n    'SVC',\n    'KNN'\n]\n\n\n\nComparing Model Performances:\nWe visualized the decision regions and calculated accuracy scores for each model. Notably, the Support Vector Classifier (SVC) stood out with a higher accuracy score and a distinct decision boundary.\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over models\nfor i, (model, model_name) in enumerate(zip(models, model_names)):\n    model.fit(X_train, y_train)\n    plot_decision_regions(X_test, y_test, clf=model, legend=2, ax=axes[i])\n\n    \n    # Set plot title\n    axes[i].set_title(f'{model_name}')\n    \n    # Display the score inside the plot\n    score = model.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nIn our quest to solve the classification problem, we initially attempted KNN but found its decision regions to be less distinct. Upon exploring alternative models, the Support Vector Classifier (SVC) emerged as a strong performer, boasting both a higher accuracy score and a more evident decision boundary. This analysis underscores the importance of model exploration and visualization in the classification process, ultimately leading to the selection of a more effective algorithm for the task at hand."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification: comparing performance of different classification models",
    "section": "",
    "text": "In this classification analysis, we aim to predict whether users will make a purchase based on their age and estimated salary. Let’s start by loading the dataset and taking a quick look at the first five rows.\n\n#Loading libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nThe dataset contains information about users, including their age, estimated salary, and whether they made a purchase (‘Purchased’ column).\n\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\nno\n\n\n1\n35\n20000\nno\n\n\n2\n26\n43000\nno\n\n\n3\n27\n57000\nno\n\n\n4\n19\n76000\nno\n\n\n\n\n\n\n\nWe are converting the ‘Purchased’ column into a binary representation for better compatibility with certain algorithms. Using the ‘apply’ method along with a lambda function, we map ‘no’ to 0 (indicating no purchase) and ‘yes’ to 1 (indicating a purchase).\n\ndf_social['Purchased']=df_social['Purchased'].apply(lambda x: 0 if x=='no' else 1)\ndf_social.head(5)\n\n\n\n\n\n\n\n\nAge\nEstimatedSalary\nPurchased\n\n\n\n\n0\n19\n19000\n0\n\n\n1\n35\n20000\n0\n\n\n2\n26\n43000\n0\n\n\n3\n27\n57000\n0\n\n\n4\n19\n76000\n0\n\n\n\n\n\n\n\n\nVisualizing the Data:\nWe start by visualizing the relationship between age, estimated salary, and purchase decisions using a scatter plot.\n\nplt.scatter(df_social[df_social['Purchased']==0]['Age'],df_social[df_social['Purchased']==0]['EstimatedSalary'], color='green', label='not purchased')\nplt.scatter(df_social[df_social['Purchased']==1]['Age'],df_social[df_social['Purchased']==1]['EstimatedSalary'], color='blue', label='purchased')\nplt.xlabel('Age')\nplt.ylabel('EstimatedSalary')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x111f2f8d0&gt;\n\n\n\n\n\n\n\nKNN Initial Attempt:\nInitially, we attempted to solve the classification problem using the K-Nearest Neighbors (KNN) algorithm. However, after viewing the decision regions, it became apparent that KNN did not provide a clear separation between the classes.\n\ny=df_social['Purchased'].values\nX=df_social.drop(['Purchased'], axis=1).values\ny=y.ravel()\n\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=4)\n\nFitting KNN model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_social=KNeighborsClassifier(n_neighbors=10)\nknn_social.fit(X_train,y_train)\nknn_social.score(X_test,y_test)\n\n0.8333333333333334\n\n\n\nfrom mlxtend.plotting import plot_decision_regions\nfig = plot_decision_regions(X_test, y_test,clf=knn_social, legend=2)\nplt.title('KNN')\nplt.show()\n\n\n\n\n\nn_neighbors_values = [4, 10, 12, 15]\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over values for n_neighbors_values\nfor i, n_neighbors in enumerate(n_neighbors_values):\n    knn_social = KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn_social.fit(X_train, y_train)\n    \n    plot_decision_regions(X_test, y_test, clf=knn_social, legend=2, ax=axes[i])\n    \n    axes[i].set_title(f'KNN, n_neighbors={n_neighbors}')\n    \n    score = knn_social.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nExploring Other Models:\nTo find a better model, we decided to explore other classification algorithms such as Decision Tree, Random Forest, and Support Vector Classifier (SVC).\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nmodels = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=10, random_state=42),\n    SVC(kernel='linear', C=1),\n    KNeighborsClassifier(n_neighbors=12)\n]\n\nmodel_names = [\n    'Decision Tree',\n    'Random Forest',\n    'SVC',\n    'KNN'\n]\n\n\n\nComparing Model Performances:\nWe visualized the decision regions and calculated accuracy scores for each model. Notably, the Support Vector Classifier (SVC) stood out with a higher accuracy score and a distinct decision boundary.\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over models\nfor i, (model, model_name) in enumerate(zip(models, model_names)):\n    model.fit(X_train, y_train)\n    plot_decision_regions(X_test, y_test, clf=model, legend=2, ax=axes[i])\n\n    \n    # Set plot title\n    axes[i].set_title(f'{model_name}')\n    \n    # Display the score inside the plot\n    score = model.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nIn our quest to solve the classification problem, we initially attempted KNN but found its decision regions to be less distinct. Upon exploring alternative models, the Support Vector Classifier (SVC) emerged as a strong performer, boasting both a higher accuracy score and a more evident decision boundary. This analysis underscores the importance of model exploration and visualization in the classification process, ultimately leading to the selection of a more effective algorithm for the task at hand."
  },
  {
    "objectID": "posts/Regression/index.html",
    "href": "posts/Regression/index.html",
    "title": "Regression: tweaking model parameter",
    "section": "",
    "text": "Introduction\nWelcome to a hands-on exploration of solving a regression problem using polynomial regression. In this scenario, we’re dealing with a classic machine learning problem – predicting salaries based on job levels. The dataset includes information about hypothetical job positions and corresponding salaries.\nProblem Statement: The goal here is to build a regression model that accurately predicts salaries. Initially, we attempt a simple linear regression model. However, if the relationship between job levels and salaries is more complex than a straight line, we need a model that can capture these nuances.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n\n\n\n\n\n\n\n\nPosition\nLevel\nSalary\n\n\n\n\n0\nBusiness Analyst\n1\n45000\n\n\n1\nJunior Consultant\n2\n50000\n\n\n2\nSenior Consultant\n3\n60000\n\n\n3\nManager\n4\n80000\n\n\n4\nCountry Manager\n5\n110000\n\n\n5\nRegion Manager\n6\n150000\n\n\n6\nPartner\n7\n200000\n\n\n7\nSenior Partner\n8\n300000\n\n\n8\nC-level\n9\n500000\n\n\n9\nCEO\n10\n1000000\n\n\n\n\n\n\n\n\n\nExploratory Data Analysis:\nBefore diving into modeling, it’s crucial to understand the data. The scatter plot visualizes the relationship between job levels and salaries, offering insights into the potential complexity of the underlying patterns.\n\nplt.scatter(df['Level'], df['Salary'])\nplt.xlabel(\"Level\")\nplt.ylabel(\"Salary\")\n\nText(0, 0.5, 'Salary')\n\n\n\n\n\n\n\nLinear Regression:\nTo start, we apply a simple linear regression model, assuming a linear relationship between job levels and salaries. The model aims to minimize the difference between actual and predicted salaries.\n\n#fit linear regression\nfrom sklearn.linear_model import LinearRegression\ny=df.iloc[:,-1].values\nX=df.iloc[:,-2:-1].values\nlin_reg=LinearRegression()\nlin_reg.fit(X,y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\nModel evaluation:\nWe evaluate the performance of the linear regression model using the R-squared metric. A low R-squared value may suggest that a linear model is insufficient for capturing the relationship in the data.\n\ny_pred = lin_reg.predict(X)\n\ndef r2(y, y_pred):\n  tss=0\n  rss=0\n  mean_true = sum(y) / len(y)\n  for true, pred in zip(y, y_pred):\n    tss+=(true-mean_true)**2\n    rss+=(true-pred)**2\n  return 1 - (rss / tss)\n\nr_sq = r2(y, y_pred)\nplt.scatter(df.Level, df.Salary, color='black')\nplt.plot(X, lin_reg.predict(X), color='black')\nplt.text(8.5, 0.75, rf'R$^2$: {r_sq:.2f}', fontsize=10, verticalalignment='top')\n\nText(8.5, 0.75, 'R$^2$: 0.67')\n\n\n\n\n\n\n\nPolynomial Regression and model evaluation:\nRecognizing potential non-linearity, we explore polynomial regression. By transforming our feature variable with different degrees, we can model curved relationships beyond what linear regression allows. We evaluate each polynomial regression model, emphasizing the importance of selecting the right degree for the polynomial features. This process allows us to uncover more complex relationships in the data.\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfig, axs = plt.subplots(3, 1, figsize=(8, 12))\n\n# polynomial regression plots with different degrees\nfor degree, ax in zip([2, 3, 4], axs):\n    # Polynomial features\n    poly_feat = PolynomialFeatures(degree=degree)\n    X_poly = poly_feat.fit_transform(X)\n    \n    # Linear regression\n    linreg_for_polyreg = LinearRegression()\n    linreg_for_polyreg.fit(X_poly, y)\n    \n    # Predicted y values\n    y_poly_pred = linreg_for_polyreg.predict(X_poly)\n    \n    # Plot\n    ax.scatter(X, y, color='black')\n    ax.plot(X, y_poly_pred, color='black')\n    r_sq=r2(y, y_poly_pred)\n    ax.set_title(f'Degree {degree} Polynomial Regression')\n    ax.text(8.5, 0.2e6, rf'R$^2$: {r_sq:.2f}', fontsize=10, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\nPredictions and Plotting:\nTo conclude, we compare predictions from both linear and polynomial regression models. This not only demonstrates the flexibility of polynomial regression but also highlights the importance of selecting the appropriate model for a given problem.\n\nx_test=[1.5, 7.8]\n\n#plotting the predicted points\nplt.scatter(X, y, label='Actual Data', color='black')\nplt.scatter(x_test, lin_reg.predict([[1.5], [7.8]]), label='Linear Regression Predictions', marker='x', color='red')\nplt.scatter(x_test, linreg_for_polyreg.predict(poly_feat.fit_transform([[1.5],[7.8]])), label='Polynomial Regression Predictions', marker='o', color='blue')\n\nplt.title('Linear and Polynomial Regression Predictions')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Clustering Analysis for Smartphone Price Range Prediction",
    "section": "",
    "text": "In this blog post, we delve into the fascinating world of clustering to predict smartphone price ranges based on various features. The dataset consists of two parts: the training set (train.csv) containing features and the categorical target variable, “price_range,” and the test set (test.csv) lacking the target column. Our objective is to categorize smartphones from the test set into different clusters using the insights gained from the training set. The dataset can be found on Kaggle\n\nimport pandas as pd\nimport numpy as np\nimport numpy as npd\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/Clustering/index.html#silhouette-analysis-understanding-cluster-quality",
    "href": "posts/Clustering/index.html#silhouette-analysis-understanding-cluster-quality",
    "title": "Clustering Analysis for Smartphone Price Range Prediction",
    "section": "Silhouette Analysis: Understanding Cluster Quality",
    "text": "Silhouette Analysis: Understanding Cluster Quality\nSilhouette analysis provides insights into the quality and separation of formed clusters. We utilize MiniBatchKMeans and visualize the silhouette scores for different cluster sizes. The code is taken from Scikit learn\n\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport matplotlib.cm as cm\nimport numpy as np\n\n\nrange_n_clusters = [2, 3, 4, 5]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.5, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X_pca) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = MiniBatchKMeans(n_clusters=n_clusters, n_init='auto', random_state=0)\n    cluster_labels = clusterer.fit_predict(X_pca)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X_pca, cluster_labels)\n    print(\n        \"For n_clusters =\",\n        n_clusters,\n        \"The average silhouette_score is :\",\n        silhouette_avg,\n    )\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X_pca, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(\n            np.arange(y_lower, y_upper),\n            0,\n            ith_cluster_silhouette_values,\n            facecolor=color,\n            edgecolor=color,\n            alpha=0.7,\n        )\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.5, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(\n        X_pca[:, 0], X_pca[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n    )\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(\n        centers[:, 0],\n        centers[:, 1],\n        marker=\"o\",\n        c=\"white\",\n        alpha=1,\n        s=200,\n        edgecolor=\"k\",\n    )\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle(\n        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n        % n_clusters,\n        fontsize=14,\n        fontweight=\"bold\",\n    )\n\nplt.show()\n\nFor n_clusters = 2 The average silhouette_score is : 0.3099031312287739\nFor n_clusters = 3 The average silhouette_score is : 0.3760007433407917\nFor n_clusters = 4 The average silhouette_score is : 0.37658001251847223\nFor n_clusters = 5 The average silhouette_score is : 0.3442148747365969"
  },
  {
    "objectID": "posts/Clustering/index.html#elbow-method-determining-optimal-cluster-count",
    "href": "posts/Clustering/index.html#elbow-method-determining-optimal-cluster-count",
    "title": "Clustering Analysis for Smartphone Price Range Prediction",
    "section": "Elbow Method: Determining Optimal Cluster Count",
    "text": "Elbow Method: Determining Optimal Cluster Count\nThe Elbow Method aids in finding the optimal number of clusters. We explore the sum of squared distances for different cluster counts to identify the ‘elbow’ point.\n\nfrom sklearn.cluster import KMeans\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nSum_of_sq_distances = []\nK = range(1,12)\nfor k in K:\n    km = KMeans(n_clusters = k)\n    km = km.fit(X_tr)\n    Sum_of_sq_distances.append(km.inertia_)\nplt.figure(figsize=(15,7))\nplt.plot(K, Sum_of_sq_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/index.html#yellowbrick-elbow-visualizer-an-alternative-visualization",
    "href": "posts/Clustering/index.html#yellowbrick-elbow-visualizer-an-alternative-visualization",
    "title": "Clustering Analysis for Smartphone Price Range Prediction",
    "section": "Yellowbrick Elbow Visualizer: An Alternative Visualization",
    "text": "Yellowbrick Elbow Visualizer: An Alternative Visualization\nThe Yellowbrick library offers an interactive visualizer to assist in determining the optimal number of clusters.\n\nfrom yellowbrick.cluster import KElbowVisualizer\nkmeans = KMeans()\nelbow = KElbowVisualizer(kmeans, k=(1, 15))\nelbow.fit(X)\nelbow.show()\nimport warnings\nwarnings.filterwarnings(\"ignore\")"
  }
]