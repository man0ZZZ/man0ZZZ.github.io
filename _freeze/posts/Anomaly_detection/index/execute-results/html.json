{
  "hash": "753849f1822c13fdb7e61fd979f273df",
  "result": {
    "markdown": "---\ntitle: Anomlay detection for a production plant\nauthor: Manoj Subedi\ndate: now\ncategories:\n  - code\n  - anomaly detection\n  - principal component analysis\n---\n\n# Introduction\n\nIn the complex realm of manufacturing, ensuring the continuous health of machinery is paramount. This blog post delves into the realm of anomaly detection within the context of eight run-to-failure experiments from a production plant. The objective is clear: identify anomalies in machine behavior using a strategic combination of feature selection, autoencoder-based clustering, and quantization errors.\n\n# Loading and Preprocessing the Datasets\n\nWe with the loading of datasets from eight run-to-failure experiments. These datasets, capturing the intricate behavior of machines, are meticulously preprocessed to align timestamps and conditions.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n```\n:::\n\n\n\n\n::: {.cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Timestamp</th>\n      <th>L_1</th>\n      <th>L_2</th>\n      <th>A_1</th>\n      <th>A_2</th>\n      <th>B_1</th>\n      <th>B_2</th>\n      <th>C_1</th>\n      <th>C_2</th>\n      <th>A_3</th>\n      <th>...</th>\n      <th>L_4</th>\n      <th>L_5</th>\n      <th>L_6</th>\n      <th>L_7</th>\n      <th>L_8</th>\n      <th>L_9</th>\n      <th>L_10</th>\n      <th>A_5</th>\n      <th>B_5</th>\n      <th>C_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>-20.470767</td>\n      <td>-49.583696</td>\n      <td>-49.737023</td>\n      <td>-60.125877</td>\n      <td>-67.684702</td>\n      <td>-67.213115</td>\n      <td>-75.635206</td>\n      <td>-75.398126</td>\n      <td>-45.568737</td>\n      <td>...</td>\n      <td>-98.610796</td>\n      <td>-77.922564</td>\n      <td>-65.288020</td>\n      <td>-21.577296</td>\n      <td>-5.229818</td>\n      <td>-20.021259</td>\n      <td>-53.399733</td>\n      <td>-100.0</td>\n      <td>-100.0</td>\n      <td>-100.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>-4.874715</td>\n      <td>-37.094569</td>\n      <td>-25.009528</td>\n      <td>-35.579886</td>\n      <td>-33.459741</td>\n      <td>-35.514754</td>\n      <td>-33.369469</td>\n      <td>-35.644538</td>\n      <td>-51.259983</td>\n      <td>...</td>\n      <td>-98.610796</td>\n      <td>-77.922564</td>\n      <td>-59.527626</td>\n      <td>-8.114661</td>\n      <td>-7.114716</td>\n      <td>-4.532685</td>\n      <td>-52.568426</td>\n      <td>-100.0</td>\n      <td>-100.0</td>\n      <td>-100.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>25.132878</td>\n      <td>-40.673543</td>\n      <td>-25.040018</td>\n      <td>-35.461876</td>\n      <td>-33.527460</td>\n      <td>-35.659016</td>\n      <td>-33.274612</td>\n      <td>-35.644538</td>\n      <td>-56.656587</td>\n      <td>...</td>\n      <td>-98.610796</td>\n      <td>-77.922564</td>\n      <td>-67.083467</td>\n      <td>22.033657</td>\n      <td>-17.198404</td>\n      <td>25.123377</td>\n      <td>-47.439417</td>\n      <td>-100.0</td>\n      <td>-100.0</td>\n      <td>-100.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>-90.508732</td>\n      <td>-76.363862</td>\n      <td>-25.238204</td>\n      <td>-35.592998</td>\n      <td>-33.459741</td>\n      <td>-35.593443</td>\n      <td>-33.396572</td>\n      <td>-35.552789</td>\n      <td>-52.903776</td>\n      <td>...</td>\n      <td>-98.610796</td>\n      <td>-77.922564</td>\n      <td>-98.546543</td>\n      <td>-85.325157</td>\n      <td>-41.011974</td>\n      <td>-91.739428</td>\n      <td>-93.192691</td>\n      <td>-100.0</td>\n      <td>-100.0</td>\n      <td>-100.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>-95.641610</td>\n      <td>-62.582329</td>\n      <td>-24.963793</td>\n      <td>-35.488101</td>\n      <td>-33.459741</td>\n      <td>-35.449180</td>\n      <td>-33.369469</td>\n      <td>-35.552789</td>\n      <td>-56.377452</td>\n      <td>...</td>\n      <td>-98.610796</td>\n      <td>-77.922564</td>\n      <td>-82.718820</td>\n      <td>-92.213349</td>\n      <td>-24.248745</td>\n      <td>-96.082302</td>\n      <td>-70.919928</td>\n      <td>-100.0</td>\n      <td>-100.0</td>\n      <td>-100.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>-95.641610</td>\n      <td>-52.914129</td>\n      <td>-24.963793</td>\n      <td>-35.474989</td>\n      <td>-33.419110</td>\n      <td>-35.580328</td>\n      <td>-33.342367</td>\n      <td>-35.592110</td>\n      <td>-53.973792</td>\n      <td>...</td>\n      <td>-98.610796</td>\n      <td>-77.922564</td>\n      <td>-82.718820</td>\n      <td>-92.213349</td>\n      <td>-24.336295</td>\n      <td>-96.082302</td>\n      <td>-67.704494</td>\n      <td>-100.0</td>\n      <td>-100.0</td>\n      <td>-100.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>-99.422931</td>\n      <td>-79.731577</td>\n      <td>-24.979038</td>\n      <td>-35.658559</td>\n      <td>-33.513916</td>\n      <td>-35.501639</td>\n      <td>-33.383021</td>\n      <td>-35.618324</td>\n      <td>-54.609599</td>\n      <td>...</td>\n      <td>-98.610796</td>\n      <td>-77.922564</td>\n      <td>-80.784439</td>\n      <td>-93.639475</td>\n      <td>-26.154242</td>\n      <td>-97.555235</td>\n      <td>-64.818446</td>\n      <td>-100.0</td>\n      <td>-100.0</td>\n      <td>-100.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>-99.422931</td>\n      <td>-79.731577</td>\n      <td>-25.161979</td>\n      <td>-35.501213</td>\n      <td>-33.378479</td>\n      <td>-35.475410</td>\n      <td>-33.355918</td>\n      <td>-56.982764</td>\n      <td>-54.547569</td>\n      <td>...</td>\n      <td>-98.610796</td>\n      <td>-77.922564</td>\n      <td>-80.784439</td>\n      <td>-93.625214</td>\n      <td>-25.438393</td>\n      <td>-97.555235</td>\n      <td>-61.101090</td>\n      <td>-100.0</td>\n      <td>-100.0</td>\n      <td>-100.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows Ã— 26 columns</p>\n</div>\n```\n:::\n:::\n\n\n\n\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nexp_names=[all_datasets[i] for i in range(len(all_datasets)) if i not in ind]\n```\n:::\n\n\n# Feature Selection: Unveiling Significance with ROC Values\n\nTo identify the most crucial features, we employ a filter method using ROC values. Decision Tree classifiers come into play, predicting machine conditions and ranking features based on univariate ROC values. Filter method of feature selection per feature is employed: build decision tree, predict the target, make prediction rank feature on the basis of machine learning metric (we will use univariate roc values in this problem)\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n# we divide total timestamps into four equal parts and label each part to describe the condition of the machine\ndef labeler(i):\n  if i<= test_df.shape[0]//4:\n    return 0\n  elif i>test_df.shape[0]//4 and i <= 2*(test_df.shape[0]//4):\n   return 1\n  elif i > 2*(test_df.shape[0]//4) and i <= 3*(test_df.shape[0]//4):\n    return 2\n  else:\n    return 3\nfor i, test_df in enumerate(datasets):\n  test_df['machine_condition']=test_df.Timestamp.apply(labeler)\n  col=test_df.pop('machine_condition')\n  test_df.insert(1, 'machine_condition', col)\n```\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\ndef roc_scores():\n  X_train, X_test, y_train, y_test=train_test_split(test_df.iloc[:,2:], test_df['machine_condition'], train_size=0.8, random_state=0)\n  # univariate roc_auc\n  df_roc_values=pd.DataFrame()\n  roc_values = []\n  for feature in X_test.columns:\n    # if feature=='L_10':\n    dtc=DecisionTreeClassifier()\n    dtc.fit(X_train[feature].to_frame(), y_train)\n    y_pred_prob=dtc.predict_proba(X_test[feature].to_frame())\n\n    #extracting roc values for multiclass output var\n    for class_idx in range(y_pred_prob.shape[1]):\n      class_roc_scores = []\n      y_true_class = (y_test == class_idx).astype(int)  # Convert to binary classification\n      y_pred_class = y_pred_prob[:, class_idx]\n      y_true_class = np.array(y_true_class)\n      y_pred_class = np.array(y_pred_class)\n      class_score=roc_auc_score(y_true_class, y_pred_class)\n      class_roc_scores.append(class_score)\n\n      # print(f\"ROC AUC Score for Class {class_idx}: {auc_score}\")\n\n    # Average ROC AUC score across all classes\n    avg_score = sum(class_roc_scores) / len(class_roc_scores)\n    roc_values.append(avg_score)\n  roc_values=pd.Series(roc_values)\n  return roc_values\n```\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndf_roc=pd.DataFrame()\nfor i, test_df in enumerate(datasets):\n  roc_series=roc_scores()\n  df_roc=pd.concat([df_roc, roc_series], axis=1)\n  # print(df_roc)\ndf_roc.index=datasets[0].iloc[:,2:].columns\ndf_roc.columns=exp_names\ndf_roc = df_roc.T\ndf_roc.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>L_1</th>\n      <th>L_2</th>\n      <th>A_1</th>\n      <th>A_2</th>\n      <th>B_1</th>\n      <th>B_2</th>\n      <th>C_1</th>\n      <th>C_2</th>\n      <th>A_3</th>\n      <th>A_4</th>\n      <th>...</th>\n      <th>L_4</th>\n      <th>L_5</th>\n      <th>L_6</th>\n      <th>L_7</th>\n      <th>L_8</th>\n      <th>L_9</th>\n      <th>L_10</th>\n      <th>A_5</th>\n      <th>B_5</th>\n      <th>C_5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>C11.csv</th>\n      <td>0.989719</td>\n      <td>0.800622</td>\n      <td>0.716381</td>\n      <td>0.805171</td>\n      <td>0.621604</td>\n      <td>0.612124</td>\n      <td>0.744775</td>\n      <td>0.715891</td>\n      <td>0.692415</td>\n      <td>0.808485</td>\n      <td>...</td>\n      <td>0.980403</td>\n      <td>0.816665</td>\n      <td>0.887565</td>\n      <td>0.963003</td>\n      <td>0.730728</td>\n      <td>0.982077</td>\n      <td>0.666120</td>\n      <td>0.625770</td>\n      <td>0.948750</td>\n      <td>0.624316</td>\n    </tr>\n    <tr>\n      <th>C13-1.csv</th>\n      <td>0.989258</td>\n      <td>0.835711</td>\n      <td>0.629530</td>\n      <td>0.790779</td>\n      <td>0.805771</td>\n      <td>0.766042</td>\n      <td>0.547736</td>\n      <td>0.768802</td>\n      <td>0.642763</td>\n      <td>0.707533</td>\n      <td>...</td>\n      <td>0.887741</td>\n      <td>0.985232</td>\n      <td>0.774679</td>\n      <td>0.635377</td>\n      <td>0.548002</td>\n      <td>0.911593</td>\n      <td>0.677480</td>\n      <td>0.670073</td>\n      <td>0.665349</td>\n      <td>0.659305</td>\n    </tr>\n    <tr>\n      <th>C14.csv</th>\n      <td>0.992529</td>\n      <td>0.781165</td>\n      <td>0.557617</td>\n      <td>0.687565</td>\n      <td>0.687223</td>\n      <td>0.689019</td>\n      <td>0.660716</td>\n      <td>0.690027</td>\n      <td>0.533338</td>\n      <td>0.686555</td>\n      <td>...</td>\n      <td>0.988066</td>\n      <td>0.894792</td>\n      <td>0.896809</td>\n      <td>0.685654</td>\n      <td>0.540606</td>\n      <td>0.904820</td>\n      <td>0.693794</td>\n      <td>0.831570</td>\n      <td>0.807473</td>\n      <td>0.852070</td>\n    </tr>\n    <tr>\n      <th>C15.csv</th>\n      <td>0.993361</td>\n      <td>0.931253</td>\n      <td>0.554749</td>\n      <td>0.858519</td>\n      <td>0.638526</td>\n      <td>0.862530</td>\n      <td>0.546365</td>\n      <td>0.662041</td>\n      <td>0.774243</td>\n      <td>0.836862</td>\n      <td>...</td>\n      <td>0.842338</td>\n      <td>0.634486</td>\n      <td>0.986360</td>\n      <td>0.649579</td>\n      <td>0.549271</td>\n      <td>0.902103</td>\n      <td>0.573459</td>\n      <td>0.571417</td>\n      <td>0.627396</td>\n      <td>0.593993</td>\n    </tr>\n    <tr>\n      <th>C16.csv</th>\n      <td>0.992478</td>\n      <td>0.842233</td>\n      <td>0.874162</td>\n      <td>0.840800</td>\n      <td>0.605960</td>\n      <td>0.840122</td>\n      <td>0.879170</td>\n      <td>0.707794</td>\n      <td>0.858725</td>\n      <td>0.817181</td>\n      <td>...</td>\n      <td>0.950300</td>\n      <td>0.795837</td>\n      <td>0.942829</td>\n      <td>0.653904</td>\n      <td>0.537704</td>\n      <td>0.926012</td>\n      <td>0.575221</td>\n      <td>0.704839</td>\n      <td>0.627433</td>\n      <td>0.703330</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 25 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# selecting features with highest average roc score across all run-to-failure experiments\ndf_roc.mean().sort_values(ascending=False)[0:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```\nL_1    0.993399\nL_9    0.937474\nL_3    0.933457\nL_4    0.927554\nL_6    0.893747\ndtype: float64\n```\n:::\n:::\n\n\n# Autoencoder for Anomaly Detection\n\nWith selected features in hand, the journey continues into the realm of autoencoders. Utilizing PCA as the foundation, we build an autoencoder to capture the essence of machine behavior.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nfor i, data in enumerate(datasets):\n  data.iloc[:,2:]=ss.fit_transform(data.iloc[:,2:])\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ndatasets_autoencoder=[]\nfor i,data in enumerate(datasets):\n  data=data.loc[:,['Timestamp','L_1','L_9','L_3','L_4','L_6']]\n  datasets_autoencoder.append(data)\n```\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\n```\n:::\n\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\n# we are not using train test split, because we want the representative data when machine is in best condition\n# so we select first 5% timestamp of every experiments\ntraining_data = np.vstack([data.iloc[:int(0.05 * len(data)),1:] for k,data in enumerate(datasets_autoencoder)])\n\ntransformed_data = pca.fit_transform(training_data)\n\n# Calculate reconstruction errors for train_data\nreconstructed_data = pca.inverse_transform(transformed_data)\nreconstruction_errors = np.mean(np.square(training_data - reconstructed_data), axis = 1)\n```\n:::\n\n\n# Setting the Anomaly Detection Threshold\n\nA crucial step in anomaly detection involves setting a threshold. We employ quantization errors to establish this threshold, a key parameter in distinguishing normal machine behavior from anomalies.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# Setting threshold for anomaly detection, in real-world is set manually based on the previous data of the machines\nthreshold = np.percentile(reconstruction_errors, 99.99)\nprint('threshold for anomaly detection:', threshold)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nthreshold for anomaly detection: 2.2508852337037815\n```\n:::\n:::\n\n\n# Anomaly Detection in Action\n\nWith the stage set, we deploy our autoencoder on all run-to-failure datasets. The quantization errors are analyzed to pinpoint timestamps where anomalies in machine behavior occur.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nfor i, data_set in enumerate(datasets_autoencoder):\n  print('                                                                    ')\n  print('                                                                    ')\n  print('=====================dataset',exp_names[i],'========================')\n  # Detect anomalies in new_data\n  transformed_data = pca.fit_transform(data_set.iloc[:,1:].values)\n  new_data_reconstructed = pca.inverse_transform(transformed_data)\n  new_data_errors = np.mean(np.square(data_set.iloc[:,1:].values - new_data_reconstructed), axis=1)\n  #print(len(new_data_errors)==data_set.shape[0])\n  print('timestamp duration: 0 to', data_set.shape[0])\n  anomaly_timestamp=[]\n  for j in range(len(new_data_errors)):\n    if new_data_errors[j] > threshold:\n      anomaly_timestamp.append(j)\n  plt.plot(range(len(data_set.index)), new_data_errors)\n  plt.axhline(y=threshold, color='red', linestyle='--', label='anomaly_threshold'.format(threshold))\n  plt.xlabel('Timestamp')\n  plt.ylabel('Quantization errors')\n  plt.title('Run-to-failure experiment '+exp_names[i])\n  plt.legend()\n  plt.show()\n\n  if len(anomaly_timestamp)<=1:\n    print('No anomaly detected')\n    continue\n  else:\n    for k in range(len(anomaly_timestamp)-1):\n      if anomaly_timestamp[k+1]-anomaly_timestamp[k]<=10:\n        print('First instance of anomaly detected at Timestamp:', anomaly_timestamp[k])\n        break\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                                                                    \n                                                                    \n=====================dataset C11.csv ========================\ntimestamp duration: 0 to 18429\nNo anomaly detected\n                                                                    \n                                                                    \n=====================dataset C13-1.csv ========================\ntimestamp duration: 0 to 23670\nFirst instance of anomaly detected at Timestamp: 18737\n                                                                    \n                                                                    \n=====================dataset C14.csv ========================\ntimestamp duration: 0 to 32848\nFirst instance of anomaly detected at Timestamp: 28477\n                                                                    \n                                                                    \n=====================dataset C15.csv ========================\ntimestamp duration: 0 to 30737\nFirst instance of anomaly detected at Timestamp: 16244\n                                                                    \n                                                                    \n=====================dataset C16.csv ========================\ntimestamp duration: 0 to 21021\nNo anomaly detected\n                                                                    \n                                                                    \n=====================dataset C7-1.csv ========================\ntimestamp duration: 0 to 51671\nFirst instance of anomaly detected at Timestamp: 31657\n                                                                    \n                                                                    \n=====================dataset C8.csv ========================\ntimestamp duration: 0 to 15803\nNo anomaly detected\n                                                                    \n                                                                    \n=====================dataset C9.csv ========================\ntimestamp duration: 0 to 34245\nFirst instance of anomaly detected at Timestamp: 13702\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-2.png){width=576 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-3.png){width=589 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-4.png){width=589 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-5.png){width=576 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-6.png){width=589 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-7.png){width=576 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-8.png){width=592 height=449}\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-17-output-9.png){width=593 height=449}\n:::\n:::\n\n\n# Conclusion\n\nThis blog post has navigated through the intricate process of anomaly detection in machine health. From feature selection using ROC values to the implementation of autoencoders and quantization errors, each step contributes to a comprehensive approach in identifying anomalies. The strategies presented here serve as a valuable toolkit for practitioners seeking to enhance machinery monitoring and predictive maintenance in industrial settings.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}