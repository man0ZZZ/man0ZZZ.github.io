{
  "hash": "cf032d22c6b7da0f99d5c3284e2e7c64",
  "result": {
    "markdown": "---\ntitle: Clustering Analysis for Smartphone Price Range Prediction\nauthor: Manoj Subedi\ndate: now\ncategories:\n  - code\n  - KMeans\n  - clustering\nformat:\n  html:\n    mermaid:\n      theme: default\n    output-file: index.html\n---\n\n# Introduction\n\nIn this blog post, we delve into the fascinating world of clustering to predict smartphone price ranges based on various features. The dataset consists of two parts: the training set (train.csv) containing features and the categorical target variable, \"price_range,\" and the test set (test.csv) lacking the target column. Our objective is to categorize smartphones from the test set into different clusters using the insights gained from the training set. The dataset can be found on [Kaggle](https://www.kaggle.com/datasets/iabhishekofficial/mobile-price-classification/)\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport numpy as npd\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n# Exploring the Datasets\n\nLet's begin by loading the datasets and exploring the first ten rows of the training set to understand the available features.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\n#load the datasets\ndf_test=pd.read_csv('./test.csv')\ndf_train=pd.read_csv('./train.csv')\n```\n:::\n\n\n::: {.cell execution_count=3}\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>battery_power</th>\n      <th>blue</th>\n      <th>clock_speed</th>\n      <th>dual_sim</th>\n      <th>fc</th>\n      <th>four_g</th>\n      <th>int_memory</th>\n      <th>m_dep</th>\n      <th>mobile_wt</th>\n      <th>n_cores</th>\n      <th>...</th>\n      <th>px_height</th>\n      <th>px_width</th>\n      <th>ram</th>\n      <th>sc_h</th>\n      <th>sc_w</th>\n      <th>talk_time</th>\n      <th>three_g</th>\n      <th>touch_screen</th>\n      <th>wifi</th>\n      <th>price_range</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>842</td>\n      <td>0</td>\n      <td>2.2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7</td>\n      <td>0.6</td>\n      <td>188</td>\n      <td>2</td>\n      <td>...</td>\n      <td>20</td>\n      <td>756</td>\n      <td>2549</td>\n      <td>9</td>\n      <td>7</td>\n      <td>19</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1021</td>\n      <td>1</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>53</td>\n      <td>0.7</td>\n      <td>136</td>\n      <td>3</td>\n      <td>...</td>\n      <td>905</td>\n      <td>1988</td>\n      <td>2631</td>\n      <td>17</td>\n      <td>3</td>\n      <td>7</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>563</td>\n      <td>1</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>41</td>\n      <td>0.9</td>\n      <td>145</td>\n      <td>5</td>\n      <td>...</td>\n      <td>1263</td>\n      <td>1716</td>\n      <td>2603</td>\n      <td>11</td>\n      <td>2</td>\n      <td>9</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>615</td>\n      <td>1</td>\n      <td>2.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>0.8</td>\n      <td>131</td>\n      <td>6</td>\n      <td>...</td>\n      <td>1216</td>\n      <td>1786</td>\n      <td>2769</td>\n      <td>16</td>\n      <td>8</td>\n      <td>11</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1821</td>\n      <td>1</td>\n      <td>1.2</td>\n      <td>0</td>\n      <td>13</td>\n      <td>1</td>\n      <td>44</td>\n      <td>0.6</td>\n      <td>141</td>\n      <td>2</td>\n      <td>...</td>\n      <td>1208</td>\n      <td>1212</td>\n      <td>1411</td>\n      <td>8</td>\n      <td>2</td>\n      <td>15</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1859</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>22</td>\n      <td>0.7</td>\n      <td>164</td>\n      <td>1</td>\n      <td>...</td>\n      <td>1004</td>\n      <td>1654</td>\n      <td>1067</td>\n      <td>17</td>\n      <td>1</td>\n      <td>10</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1821</td>\n      <td>0</td>\n      <td>1.7</td>\n      <td>0</td>\n      <td>4</td>\n      <td>1</td>\n      <td>10</td>\n      <td>0.8</td>\n      <td>139</td>\n      <td>8</td>\n      <td>...</td>\n      <td>381</td>\n      <td>1018</td>\n      <td>3220</td>\n      <td>13</td>\n      <td>8</td>\n      <td>18</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1954</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>24</td>\n      <td>0.8</td>\n      <td>187</td>\n      <td>4</td>\n      <td>...</td>\n      <td>512</td>\n      <td>1149</td>\n      <td>700</td>\n      <td>16</td>\n      <td>3</td>\n      <td>5</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1445</td>\n      <td>1</td>\n      <td>0.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>53</td>\n      <td>0.7</td>\n      <td>174</td>\n      <td>7</td>\n      <td>...</td>\n      <td>386</td>\n      <td>836</td>\n      <td>1099</td>\n      <td>17</td>\n      <td>1</td>\n      <td>20</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>509</td>\n      <td>1</td>\n      <td>0.6</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>9</td>\n      <td>0.1</td>\n      <td>93</td>\n      <td>5</td>\n      <td>...</td>\n      <td>1137</td>\n      <td>1224</td>\n      <td>513</td>\n      <td>19</td>\n      <td>10</td>\n      <td>12</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows Ã— 21 columns</p>\n</div>\n```\n:::\n:::\n\n\n# Feature Engineering: Filtering the Noise\n\nWith numerous features at our disposal, the dataset can be noisy, potentially affecting the quality of our clustering. To address this, we employ a simple filter method using ANOVA (Analysis of Variance) to select the most important features.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndf = df_train\nall_col = [x for x in df_train.columns]\nall_col = all_col[:-1]\ny = df['price_range']\ncat_col=['blue','three_g', 'dual_sim', 'four_g', 'wifi', 'touch_screen']\n```\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nanova_dict = {}\nfor col in all_col:\n  if col not in cat_col:\n    for i in df['price_range'].unique():\n      var_name = f\"feat_{col}_{i}\"\n      var_value = df[df['price_range']==i][col].values\n      anova_dict[var_name]=var_value\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom scipy.stats import f_oneway\nf_stat_all = []\np_val_all = []\nfor col in all_col:\n  keys_all = []\n  for key, value in anova_dict.items():\n    if str(col) in str(key):\n      keys_all.append(key)\n  if len(keys_all) > 0:\n    f_stat, p_val = f_oneway(anova_dict[keys_all[0]], anova_dict[keys_all[1]], anova_dict[keys_all[2]], anova_dict[keys_all[3]])\n    f_stat_all.append(f_stat)\n    p_val_all.append(p_val)\n```\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nanova_table = pd.DataFrame([[col for col in all_col if col not in cat_col], f_stat_all, p_val_all], index=['features', 'f_statistic', 'p_value']).T\nanova_table\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>features</th>\n      <th>f_statistic</th>\n      <th>p_value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>battery_power</td>\n      <td>31.598158</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>clock_speed</td>\n      <td>0.493708</td>\n      <td>0.686675</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>fc</td>\n      <td>0.772182</td>\n      <td>0.509504</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>int_memory</td>\n      <td>2.922996</td>\n      <td>0.032777</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>m_dep</td>\n      <td>1.500682</td>\n      <td>0.212459</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>mobile_wt</td>\n      <td>3.594318</td>\n      <td>0.013117</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>n_cores</td>\n      <td>2.625415</td>\n      <td>0.048936</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>pc</td>\n      <td>0.825446</td>\n      <td>0.479749</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>px_height</td>\n      <td>19.484842</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>px_width</td>\n      <td>22.620882</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>ram</td>\n      <td>3520.110824</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>sc_h</td>\n      <td>2.225984</td>\n      <td>0.08325</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>sc_w</td>\n      <td>1.671</td>\n      <td>0.171215</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>talk_time</td>\n      <td>1.628811</td>\n      <td>0.180669</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nselected_features=[x for x in anova_table[anova_table['p_value'] < 0.05]['features'].values]\nselected_features\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n['battery_power',\n 'int_memory',\n 'mobile_wt',\n 'n_cores',\n 'px_height',\n 'px_width',\n 'ram']\n```\n:::\n:::\n\n\n# Data Preprocessing: Scaling and Dimensionality Reduction\n\nBefore applying clustering algorithms, it's crucial to preprocess the data. We use Min-Max scaling to normalize the selected features and then apply Principal Component Analysis (PCA) for dimensionality reduction.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndf = df_test.loc[:,selected_features]\nfrom sklearn.preprocessing import MinMaxScaler\nmms = MinMaxScaler()\nX = df.values\nX_tr = mms.fit_transform(X)\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_tr)\ndf_pca = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n```\n:::\n\n\n# Clustering and Evaluation\n\n## Silhouette Analysis: Understanding Cluster Quality\n\nSilhouette analysis provides insights into the quality and separation of formed clusters. We utilize MiniBatchKMeans and visualize the silhouette scores for different cluster sizes. The code is taken from [Scikit learn](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport matplotlib.cm as cm\nimport numpy as np\n```\n:::\n\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nrange_n_clusters = [2, 3, 4, 5]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.5, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X_pca) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = MiniBatchKMeans(n_clusters=n_clusters, n_init='auto', random_state=0)\n    cluster_labels = clusterer.fit_predict(X_pca)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X_pca, cluster_labels)\n    print(\n        \"For n_clusters =\",\n        n_clusters,\n        \"The average silhouette_score is :\",\n        silhouette_avg,\n    )\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X_pca, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) / n_clusters)\n        ax1.fill_betweenx(\n            np.arange(y_lower, y_upper),\n            0,\n            ith_cluster_silhouette_values,\n            facecolor=color,\n            edgecolor=color,\n            alpha=0.7,\n        )\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n    ax1.set_xticks([-0.5, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    # 2nd Plot showing the actual clusters formed\n    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n    ax2.scatter(\n        X_pca[:, 0], X_pca[:, 1], marker=\".\", s=30, lw=0, alpha=0.7, c=colors, edgecolor=\"k\"\n    )\n\n    # Labeling the clusters\n    centers = clusterer.cluster_centers_\n    # Draw white circles at cluster centers\n    ax2.scatter(\n        centers[:, 0],\n        centers[:, 1],\n        marker=\"o\",\n        c=\"white\",\n        alpha=1,\n        s=200,\n        edgecolor=\"k\",\n    )\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker=\"$%d$\" % i, alpha=1, s=50, edgecolor=\"k\")\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle(\n        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n        % n_clusters,\n        fontsize=14,\n        fontweight=\"bold\",\n    )\n\nplt.show()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nFor n_clusters = 2 The average silhouette_score is : 0.309903131228774\nFor n_clusters = 3 The average silhouette_score is : 0.3760007433407918\nFor n_clusters = 4 The average silhouette_score is : 0.37658001251847256\nFor n_clusters = 5 The average silhouette_score is : 0.3442148747365973\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-13-output-2.png){width=1377 height=645}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-13-output-3.png){width=1377 height=645}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-13-output-4.png){width=1377 height=645}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-13-output-5.png){width=1377 height=645}\n:::\n:::\n\n\n## Elbow Method: Determining Optimal Cluster Count\n\nThe Elbow Method aids in finding the optimal number of clusters. We explore the sum of squared distances for different cluster counts to identify the 'elbow' point.\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nfrom sklearn.cluster import KMeans\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nSum_of_sq_distances = []\nK = range(1,12)\nfor k in K:\n    km = KMeans(n_clusters = k)\n    km = km.fit(X_tr)\n    Sum_of_sq_distances.append(km.inertia_)\nplt.figure(figsize=(15,7))\nplt.plot(K, Sum_of_sq_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-14-output-1.png){width=1188 height=597}\n:::\n:::\n\n\n## Yellowbrick Elbow Visualizer: An Alternative Visualization\n\nThe Yellowbrick library offers an interactive visualizer to assist in determining the optimal number of clusters.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nfrom yellowbrick.cluster import KElbowVisualizer\nkmeans = KMeans()\nelbow = KElbowVisualizer(kmeans, k=(1, 15))\nelbow.fit(X)\nelbow.show()\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-15-output-1.png){width=716 height=486}\n:::\n:::\n\n\n# Final KMeans Clustering: Bringing It All Together\n\nHaving explored and evaluated different aspects, we apply KMeans clustering with the determined optimal number of clusters.\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nkm = KMeans(n_clusters = 3)\nkm.fit(X_tr)\ndf['Cluster'] = km.labels_\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=df['Cluster'], cmap='viridis')\nplt.title('Scatter Plot with KMeans Clusters')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Clustering_files/figure-html/cell-16-output-1.png){width=677 height=486}\n:::\n:::\n\n\nIn conclusion, this blog post has unraveled the process of categorizing smartphones through a machine learning lens. From feature engineering to clustering evaluation, each step contributes to our understanding of the dataset and aids in efficient categorization. The exploration of various metrics and visualization techniques offers a comprehensive approach to solving this clustering problem, shedding light on the inherent patterns within the smartphone dataset.\n\n",
    "supporting": [
      "Clustering_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}