{
  "hash": "42b2c46d6ac55a13389037d1657c725c",
  "result": {
    "markdown": "---\ntitle: Leveraging Odds Ratios for Advanced Keyword Analysis in Email Categorization\nauthor: Manoj Subedi\ndate: now\ncategories:\n  - code\n---\n\n# Introduction\n\nIn the vast realm of email communication, distinguishing between legitimate (ham) and unwanted (spam) messages is a constant challenge. Leveraging machine learning and statistical analysis, this blog post embarks on a journey to identify crucial keywords that significantly contribute to the categorization of emails. The ultimate goal is to uncover features that enhance the accuracy of models in predicting spam emails.\n\n# Loading and Preprocessing the Email Dataset\n\nOur exploration begins by loading the email dataset and preparing it for analysis. Each email is labeled as either 'ham' or 'spam', and we aim to extract valuable insights from the text content.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv('./spamhamdata.csv', sep = '\\t', header=None)\ndf.columns = ['email_type', 'text']\ndf['label']=df['email_type'].apply(lambda x: 0 if x == 'ham' else 1)\ndf.head(7)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>email_type</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>spam</td>\n      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. ...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# Text Cleaning and Keyword Extraction\n\nTo identify important keywords, we perform text cleaning by converting words to lowercase, removing numbers, punctuation, and extra spaces. Additionally, we apply lemmatization to obtain the base form of words and remove common English stop words.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport re\nimport nltk\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ndef clean_text(sentence):\n\n  # Converting all words to lower\n  sentence_lowered=sentence.lower()\n\n  # removing puntuations and numbers using regular expression\n  sentence_no_numbers = re.sub(r'\\d+', '', sentence_lowered)\n  sentence_no_punctuations = re.sub(r'[^\\w\\s]', '', sentence_no_numbers)\n  sentence_no_extra_spaces = re.sub(' +', ' ', sentence_no_punctuations)\n\n  # fetched words from nltk package\n  stop = stopwords.words('english')\n\n  stop_words_removed_sentence=[]\n  for k in sentence_no_extra_spaces.split(\" \"):\n    if k not in stop:\n      stop_words_removed_sentence = [lemmatizer.lemmatize(word) for word in sentence_no_extra_spaces.split() if word not in stop and len(word) > 2]\n  return stop_words_removed_sentence\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/apolloos/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     /Users/apolloos/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n```\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\n# example of text cleaning\ntext_sample=\"I am..  ..  $so angry.. that a cu i made this post available\"\nclean_text(text_sample)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n['angry', 'made', 'post', 'available']\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndf['keywords']=df['text'].apply(clean_text)\ndf.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>email_type</th>\n      <th>text</th>\n      <th>label</th>\n      <th>keywords</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>0</td>\n      <td>[jurong, point, crazy, available, bugis, great...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>0</td>\n      <td>[lar, joking, wif, oni]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>1</td>\n      <td>[free, entry, wkly, comp, win, cup, final, tkt...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>0</td>\n      <td>[dun, say, early, hor, already, say]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>0</td>\n      <td>[nah, dont, think, go, usf, life, around, though]</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# Frequency Analysis of Keywords\n\nWe analyze the frequency of each word and identify a set of highly repeated words. These words serve as potential candidates for contributing to the categorization process.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n#getting the most frequently repeated words\n#| echo: false\nall_words=[]\nfor i in df['keywords']:\n  for j in i:\n    all_words.append(j)\ncount =[]\nfor i in all_words:\n  count.append(all_words.count(i))\ndf_words_count = pd.DataFrame([all_words, count], index = ['words', 'count']).T\ndf_words_count.drop_duplicates(inplace=True)\ndf_words_count.sort_values('count', ascending=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=6}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>words</th>\n      <th>count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>93</th>\n      <td>call</td>\n      <td>605</td>\n    </tr>\n    <tr>\n      <th>410</th>\n      <td>get</td>\n      <td>401</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>dont</td>\n      <td>298</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>free</td>\n      <td>278</td>\n    </tr>\n    <tr>\n      <th>405</th>\n      <td>ltgt</td>\n      <td>276</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>16257</th>\n      <td>visitor</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16241</th>\n      <td>forwarding</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16239</th>\n      <td>brilliantly</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16226</th>\n      <td>outreach</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>44521</th>\n      <td>bitching</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>7760 rows Ã— 2 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\n#selecting 149 highly repeated words\ndf_high_freq = df_words_count[df_words_count['count'] > 50]\nhigh_freq_word = list(df_high_freq['words'])\nlen(df_high_freq.index)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n149\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\n# counting instances of occurence of highly repeated words in ham and spam emails\nham=[]\nspam=[]\nfor word in high_freq_word:\n  ham_count=0\n  spam_count=0\n  for ind in range(len(df)):\n    current_row = df.iloc[ind]\n    if word in current_row['keywords']:\n      if current_row['label']==0:\n        ham_count+=1\n      else:\n        spam_count+=1\n  #print(word,ham_count,spam_count)\n  ham.append(ham_count)\n  spam.append(spam_count)\ndf_odds = pd.DataFrame([high_freq_word, ham, spam], index= ['keywords','ham_count','spam_count']).T\ndf_odds = df_odds[df_odds['spam_count'] != 0]\ndf_odds.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keywords</th>\n      <th>ham_count</th>\n      <th>spam_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>great</td>\n      <td>93</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>got</td>\n      <td>222</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>wat</td>\n      <td>91</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>free</td>\n      <td>58</td>\n      <td>169</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>win</td>\n      <td>12</td>\n      <td>62</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# Odds Ratio and Significance Testing: A Statistical Lens\n\nNow, we delve into the statistical significance of each keyword using odds ratio and p-values. The odds ratio provides insights into the likelihood of a keyword occurring in spam compared to ham emails. Simultaneously, p-values quantify the evidence against a null hypothesis, helping us assess the significance of the observed differences\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nimport pandas as pd\nfrom scipy.stats import fisher_exact\n\n# Calculate odds ratio and p-value for each word\nodds_ratios = []\np_values = []\n\nfor index, row in df_odds.iterrows():\n    contingency_table = [\n        [row['spam_count'], row['ham_count']],\n        [len(df.index) - row['spam_count'], len(df.index) - row['ham_count']]\n    ]\n\n    odds_ratio, p_value = fisher_exact(contingency_table)\n    odds_ratios.append(odds_ratio)\n    p_values.append(p_value)\n\n# Add results to the data frame\ndf_odds['Odds_Ratio'] = odds_ratios\ndf_odds['P_Value'] = p_values\n\n# Display the updated data frame\ndf_odds.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=9}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>keywords</th>\n      <th>ham_count</th>\n      <th>spam_count</th>\n      <th>Odds_Ratio</th>\n      <th>P_Value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>great</td>\n      <td>93</td>\n      <td>11</td>\n      <td>0.116535</td>\n      <td>1.838932e-17</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>got</td>\n      <td>222</td>\n      <td>7</td>\n      <td>0.030313</td>\n      <td>1.735920e-57</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>wat</td>\n      <td>91</td>\n      <td>1</td>\n      <td>0.010811</td>\n      <td>2.614224e-26</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>free</td>\n      <td>58</td>\n      <td>169</td>\n      <td>2.973654</td>\n      <td>5.124485e-14</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>win</td>\n      <td>12</td>\n      <td>62</td>\n      <td>5.213551</td>\n      <td>2.553913e-09</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# Benjamini-Hochberg Correction and Visualization\n\nThe p-value tests the hypothesis that the odds of the keyword occurring in either ham or spam emails are equal (i.e., 1.0). However, the p-values can easily provide misleading results for two reasons: The p-value does not measure the magnitude of the differences and it raises the risk of false positive rate. To address the issue of false positives, we apply the Benjamini-Hochberg correction. We then visualize the results using a volcano plot, highlighting keywords with substantial odds ratios and low corrected p-values.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Apply Benjamini-Hochberg correction\nfrom statsmodels.stats.multitest import multipletests\n_, corrected_p_values, _, _ = multipletests(df_odds['P_Value'], method='fdr_bh')\ndf_odds['FDR_Corrected_P_Value'] = corrected_p_values\n```\n:::\n\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\n# Create a volcano plot\nplt.figure(figsize=(10, 6))\nplt.scatter(df_odds['Odds_Ratio'], -1 * np.log10(df_odds['FDR_Corrected_P_Value']), color='blue')\n\n# Add labels and title\nplt.xlabel('Odds Ratio')\nplt.ylabel('-log10(FDR Corrected P-Value)')\nplt.title('Volcano Plot of the Keyword analysis')\n\n# Show the plot\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=808 height=523}\n:::\n:::\n\n\n# Final Selection of Keywords\n\nBy setting criteria for importance, we narrow down the selection to keywords with odds ratios greater than 5 and FDR-corrected p-values less than 10\\^-5. These selected keywords can serve as impactful features for enhancing the accuracy of email categorization models.\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\ndf_final = df_odds[(df_odds['Odds_Ratio'] > 5) & (df_odds['FDR_Corrected_P_Value'] < 10**-5)]\nlist(df_final['keywords'])\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n['win',\n 'txt',\n 'customer',\n 'prize',\n 'claim',\n 'mobile',\n 'cash',\n 'urgent',\n 'nokia',\n 'service',\n 'box',\n 'tone']\n```\n:::\n:::\n\n\n# Conclusion\n\nIn conclusion, the application of odds ratios and p-values provides a robust statistical foundation for keyword selection. These measures not only quantify the magnitude of differences but also mitigate the risk of false positives. The selected keywords, enriched with statistical significance, can now be employed as powerful features to enhance the accuracy and predictive capabilities of machine learning models in identifying spam emails. Source: [Feature Engineering and Selection: A Practical Approach for Predictive Models](http://www.feat.engineering/text-data)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}