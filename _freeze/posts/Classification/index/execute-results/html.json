{
  "hash": "935594192949b88a65760a020d266e73",
  "result": {
    "markdown": "---\ntitle: 'Classification: comparing performance of different classification models'\nauthor: Manoj Subedi\ndate: now\ncategories:\n  - code\n---\n\nIn this classification analysis, we aim to predict whether users will make a purchase based on their age and estimated salary. Let's start by loading the dataset and taking a quick look at the first five rows.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n#Loading libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n```\n:::\n\n\nThe dataset contains information about users, including their age, estimated salary, and whether they made a purchase ('Purchased' column).\n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>EstimatedSalary</th>\n      <th>Purchased</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19</td>\n      <td>19000</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>35</td>\n      <td>20000</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26</td>\n      <td>43000</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>27</td>\n      <td>57000</td>\n      <td>no</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>19</td>\n      <td>76000</td>\n      <td>no</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe are converting the 'Purchased' column into a binary representation for better compatibility with certain algorithms. Using the 'apply' method along with a lambda function, we map 'no' to 0 (indicating no purchase) and 'yes' to 1 (indicating a purchase).\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf_social['Purchased']=df_social['Purchased'].apply(lambda x: 0 if x=='no' else 1)\ndf_social.head(5)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Age</th>\n      <th>EstimatedSalary</th>\n      <th>Purchased</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19</td>\n      <td>19000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>35</td>\n      <td>20000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26</td>\n      <td>43000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>27</td>\n      <td>57000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>19</td>\n      <td>76000</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n# Visualizing the Data:\n\nWe start by visualizing the relationship between age, estimated salary, and purchase decisions using a scatter plot.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nplt.scatter(df_social[df_social['Purchased']==0]['Age'],df_social[df_social['Purchased']==0]['EstimatedSalary'], color='green', label='not purchased')\nplt.scatter(df_social[df_social['Purchased']==1]['Age'],df_social[df_social['Purchased']==1]['EstimatedSalary'], color='blue', label='purchased')\nplt.xlabel('Age')\nplt.ylabel('EstimatedSalary')\nplt.legend()\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n<matplotlib.legend.Legend at 0x111f2f8d0>\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-2.png){width=619 height=429}\n:::\n:::\n\n\n# KNN Initial Attempt:\n\nInitially, we attempted to solve the classification problem using the K-Nearest Neighbors (KNN) algorithm. However, after viewing the decision regions, it became apparent that KNN did not provide a clear separation between the classes.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ny=df_social['Purchased'].values\nX=df_social.drop(['Purchased'], axis=1).values\ny=y.ravel()\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=4)\n```\n:::\n\n\nFitting KNN model\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom sklearn.neighbors import KNeighborsClassifier\nknn_social=KNeighborsClassifier(n_neighbors=10)\nknn_social.fit(X_train,y_train)\nknn_social.score(X_test,y_test)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\n0.8333333333333334\n```\n:::\n:::\n\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom mlxtend.plotting import plot_decision_regions\nfig = plot_decision_regions(X_test, y_test,clf=knn_social, legend=2)\nplt.title('KNN')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=609 height=431}\n:::\n:::\n\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nn_neighbors_values = [4, 10, 12, 15]\n\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over values for n_neighbors_values\nfor i, n_neighbors in enumerate(n_neighbors_values):\n    knn_social = KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn_social.fit(X_train, y_train)\n    \n    plot_decision_regions(X_test, y_test, clf=knn_social, legend=2, ax=axes[i])\n    \n    axes[i].set_title(f'KNN, n_neighbors={n_neighbors}')\n    \n    score = knn_social.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=758 height=757}\n:::\n:::\n\n\n# Exploring Other Models:\n\nTo find a better model, we decided to explore other classification algorithms such as Decision Tree, Random Forest, and Support Vector Classifier (SVC).\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nmodels = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(n_estimators=10, random_state=42),\n    SVC(kernel='linear', C=1),\n    KNeighborsClassifier(n_neighbors=12)\n]\n\nmodel_names = [\n    'Decision Tree',\n    'Random Forest',\n    'SVC',\n    'KNN'\n]\n```\n:::\n\n\n# Comparing Model Performances:\n\nWe visualized the decision regions and calculated accuracy scores for each model. Notably, the Support Vector Classifier (SVC) stood out with a higher accuracy score and a distinct decision boundary.\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nfig, axes = plt.subplots(2, 2, figsize=(8, 8))\naxes = axes.flatten()\n\n# Iterate over models\nfor i, (model, model_name) in enumerate(zip(models, model_names)):\n    model.fit(X_train, y_train)\n    plot_decision_regions(X_test, y_test, clf=model, legend=2, ax=axes[i])\n\n    \n    # Set plot title\n    axes[i].set_title(f'{model_name}')\n    \n    # Display the score inside the plot\n    score = model.score(X_test, y_test)\n    axes[i].text(0.75, 0.75, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')\n\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-12-output-1.png){width=758 height=757}\n:::\n:::\n\n\nIn our quest to solve the classification problem, we initially attempted KNN but found its decision regions to be less distinct. Upon exploring alternative models, the Support Vector Classifier (SVC) emerged as a strong performer, boasting both a higher accuracy score and a more evident decision boundary. This analysis underscores the importance of model exploration and visualization in the classification process, ultimately leading to the selection of a more effective algorithm for the task at hand.\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}