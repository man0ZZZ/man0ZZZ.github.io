---
title: 'Classification: comparing performance of different classification models'
author: Manoj Subedi
date: now
categories:
  - code
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.15.2
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

In this classification analysis, we aim to predict whether users will make a purchase based on their age and estimated salary.  Let's start by loading the dataset and taking a quick look at the first five rows.

```{python}
#Loading libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
```

The dataset contains information about users, including their age, estimated salary, and whether they made a purchase ('Purchased' column).

```{python}
df_social=pd.read_csv('./Social_Network_Ads.csv')
df_social.head(5)
```

We are converting the 'Purchased' column into a binary representation for better compatibility with certain algorithms. Using the 'apply' method along with a lambda function, we map 'no' to 0 (indicating no purchase) and 'yes' to 1 (indicating a purchase).

```{python}
df_social['Purchased']=df_social['Purchased'].apply(lambda x: 0 if x=='no' else 1)
df_social.head(5)
```

# Visualizing the Data:

We start by visualizing the relationship between age, estimated salary, and purchase decisions using a scatter plot.

```{python}
#| jupyter: {outputs_hidden: true}
plt.scatter(df_social[df_social['Purchased']==0]['Age'],df_social[df_social['Purchased']==0]['EstimatedSalary'], color='green', label='not purchased')
plt.scatter(df_social[df_social['Purchased']==1]['Age'],df_social[df_social['Purchased']==1]['EstimatedSalary'], color='blue', label='purchased')
plt.xlabel('Age')
plt.ylabel('EstimatedSalary')
plt.legend()
```

# KNN Initial Attempt:

Initially, we attempted to solve the classification problem using the K-Nearest Neighbors (KNN) algorithm. However, after viewing the decision regions, it became apparent that KNN did not provide a clear separation between the classes.

```{python}
X=df_social.drop(['Purchased'], axis=1).values
y=df_social.iloc[:,-2:-1].values
y=y.ravel()
```

```{python}
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=4)
```

Fitting KNN model

```{python}
from sklearn.neighbors import KNeighborsClassifier
knn_social=KNeighborsClassifier(n_neighbors=10)
knn_social.fit(X_train,y_train)
knn_social.score(X_test,y_test)
```

```{python}
#| jupyter: {outputs_hidden: true}
from mlxtend.plotting import plot_decision_regions
fig = plot_decision_regions(X_test, y_test,clf=knn_social, legend=2)
plt.title('KNN')
plt.show()
```

```{python}
#| jupyter: {outputs_hidden: true}
n_neighbors_values = [4, 10, 12, 15]

fig, axes = plt.subplots(2, 2, figsize=(10, 8))
axes = axes.flatten()

# Iterate over values for n_neighbors_values
for i, n_neighbors in enumerate(n_neighbors_values):
    knn_social = KNeighborsClassifier(n_neighbors=n_neighbors)
    knn_social.fit(X_train, y_train)
    
    plot_decision_regions(X_test, y_test, clf=knn_social, legend=2, ax=axes[i])
    
    axes[i].set_title(f'KNN, n_neighbors={n_neighbors}')
    
    score = knn_social.score(X_test, y_test)
    axes[i].text(0.85, 0.9, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')

plt.tight_layout()
plt.show()
```

# Exploring Other Models:

To find a better model, we decided to explore other classification algorithms such as Decision Tree, Random Forest, and Support Vector Classifier (SVC).

```{python}
#| jupyter: {outputs_hidden: true}
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
models = [
    DecisionTreeClassifier(),
    RandomForestClassifier(n_estimators=10, random_state=42),
    SVC(kernel='linear', C=1),
    KNeighborsClassifier(n_neighbors=12)
]

model_names = [
    'Decision Tree',
    'Random Forest',
    'SVC',
    'KNN'
]
```

# Comparing Model Performances:

We visualized the decision regions and calculated accuracy scores for each model. Notably, the Support Vector Classifier (SVC) stood out with a higher accuracy score and a distinct decision boundary.

```{python}
#| jupyter: {outputs_hidden: true}
fig, axes = plt.subplots(2, 2, figsize=(10, 8))
axes = axes.flatten()

# Iterate over models
for i, (model, model_name) in enumerate(zip(models, model_names)):
    model.fit(X_train, y_train)
    plot_decision_regions(X_test, y_test, clf=model, legend=2, ax=axes[i])

    
    # Set plot title
    axes[i].set_title(f'{model_name}')
    
    # Display the score inside the plot
    score = model.score(X_test, y_test)
    axes[i].text(0.85, 0.9, f'Score: {score:.2f}', transform=axes[i].transAxes, fontsize=10, verticalalignment='top')


plt.tight_layout()
plt.show()
```

In our quest to solve the classification problem, we initially attempted KNN but found its decision regions to be less distinct. Upon exploring alternative models, the Support Vector Classifier (SVC) emerged as a strong performer, boasting both a higher accuracy score and a more evident decision boundary. This analysis underscores the importance of model exploration and visualization in the classification process, ultimately leading to the selection of a more effective algorithm for the task at hand.
